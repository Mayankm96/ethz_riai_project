{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../ELINA/python_interface/')\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from elina_box import *\n",
    "from elina_interval import *\n",
    "from elina_abstract0 import *\n",
    "from elina_manager import *\n",
    "from elina_dimension import *\n",
    "from elina_scalar import *\n",
    "from elina_interval import *\n",
    "from elina_linexpr0 import *\n",
    "from elina_lincons0 import *\n",
    "import ctypes\n",
    "from ctypes.util import find_library\n",
    "from gurobipy import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "libc = CDLL(find_library('c'))\n",
    "cstdout = c_void_p.in_dll(libc, 'stdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for debugging in jupyter notebook\n",
    "from IPython.core.debugger import set_trace #TODO remove at end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layers:\n",
    "    def __init__(self):\n",
    "        self.layertypes = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.numlayer = 0\n",
    "        self.ffn_counter = 0\n",
    "        self.rank = []\n",
    "        self.use_LP = []\n",
    "        self.LB_hat = []\n",
    "        self.UB_hat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bias(text):\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    v = np.array([*map(lambda x: np.double(x.strip()), text[1:-1].split(','))])\n",
    "    #return v.reshape((v.size,1))\n",
    "    return v\n",
    "\n",
    "def parse_vector(text):\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    v = np.array([*map(lambda x: np.double(x.strip()), text[1:-1].split(','))])\n",
    "    return v.reshape((v.size,1))\n",
    "    #return v\n",
    "    \n",
    "def balanced_split(text):\n",
    "    i = 0\n",
    "    bal = 0\n",
    "    start = 0\n",
    "    result = []\n",
    "    while i < len(text):\n",
    "        if text[i] == '[':\n",
    "            bal += 1\n",
    "        elif text[i] == ']':\n",
    "            bal -= 1\n",
    "        elif text[i] == ',' and bal == 0:\n",
    "            result.append(text[start:i])\n",
    "            start = i+1\n",
    "        i += 1\n",
    "    if start < i:\n",
    "        result.append(text[start:i])\n",
    "    return result\n",
    "\n",
    "def parse_matrix(text):\n",
    "    i = 0\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    return np.array([*map(lambda x: parse_vector(x.strip()).flatten(), balanced_split(text[1:-1]))])\n",
    "\n",
    "def parse_net(text):\n",
    "    lines = [*filter(lambda x: len(x) != 0, text.split('\\n'))]\n",
    "    i = 0\n",
    "    res = layers()\n",
    "    while i < len(lines):\n",
    "        if lines[i] in ['ReLU', 'Affine']:\n",
    "            W = parse_matrix(lines[i+1])\n",
    "            b = parse_bias(lines[i+2])\n",
    "            res.layertypes.append(lines[i])\n",
    "            res.weights.append(W)\n",
    "            res.biases.append(b)\n",
    "            res.numlayer+= 1\n",
    "            res.rank.append(np.zeros((W.shape[0],1)))\n",
    "            res.use_LP.append(np.full((W.shape[0],1), False))\n",
    "            res.LB_hat.append(np.full((W.shape[0],1), np.nan))\n",
    "            res.UB_hat.append(np.full((W.shape[0],1), np.nan))\n",
    "            i += 3\n",
    "        else:\n",
    "            raise Exception('parse error: '+lines[i])\n",
    "    return res\n",
    "\n",
    "def parse_spec(text):\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    with open('dummy', 'w') as my_file:\n",
    "        my_file.write(text)\n",
    "    data = np.genfromtxt('dummy', delimiter=',',dtype=np.double)\n",
    "    low = copy.deepcopy(data[:,0])\n",
    "    high = copy.deepcopy(data[:,1])\n",
    "    return low,high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perturbed_image(x, epsilon):\n",
    "    image = x[1:len(x)]\n",
    "    num_pixels = len(image)\n",
    "    LB_N0 = image - epsilon\n",
    "    UB_N0 = image + epsilon\n",
    "     \n",
    "    for i in range(num_pixels):\n",
    "        if(LB_N0[i] < 0):\n",
    "            LB_N0[i] = 0\n",
    "        if(UB_N0[i] > 1):\n",
    "            UB_N0[i] = 1\n",
    "    return LB_N0, UB_N0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linexpr0(weights, bias, size):\n",
    "    linexpr0 = elina_linexpr0_alloc(ElinaLinexprDiscr.ELINA_LINEXPR_DENSE, size)\n",
    "    cst = pointer(linexpr0.contents.cst)\n",
    "    elina_scalar_set_double(cst.contents.val.scalar, bias)\n",
    "    for i in range(size):\n",
    "        elina_linexpr0_set_coeff_scalar_double(linexpr0,i,weights[i])\n",
    "    return linexpr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(nn, LB_N0, UB_N0, label):   \n",
    "    num_pixels = len(LB_N0)\n",
    "    nn.ffn_counter = 0\n",
    "    numlayer = nn.numlayer \n",
    "    man = elina_box_manager_alloc()\n",
    "    itv = elina_interval_array_alloc(num_pixels)\n",
    "    for i in range(num_pixels):\n",
    "        elina_interval_set_double(itv[i],LB_N0[i],UB_N0[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_pixels, itv)\n",
    "    elina_interval_array_free(itv,num_pixels)\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "           weights = nn.weights[nn.ffn_counter]\n",
    "           biases = nn.biases[nn.ffn_counter]\n",
    "           dims = elina_abstract0_dimension(man,element)\n",
    "           num_in_pixels = dims.intdim + dims.realdim\n",
    "           num_out_pixels = len(weights)\n",
    "\n",
    "           dimadd = elina_dimchange_alloc(0,num_out_pixels)    \n",
    "           for i in range(num_out_pixels):\n",
    "               dimadd.contents.dim[i] = num_in_pixels\n",
    "           elina_abstract0_add_dimensions(man, True, element, dimadd, False)\n",
    "           elina_dimchange_free(dimadd)\n",
    "           np.ascontiguousarray(weights, dtype=np.double)\n",
    "           np.ascontiguousarray(biases, dtype=np.double)\n",
    "           var = num_in_pixels\n",
    "           # handle affine layer\n",
    "           for i in range(num_out_pixels):\n",
    "               tdim= ElinaDim(var)\n",
    "               linexpr0 = generate_linexpr0(weights[i],biases[i],num_in_pixels)\n",
    "               element = elina_abstract0_assign_linexpr_array(man, True, element, tdim, linexpr0, 1, None)\n",
    "               var+=1\n",
    "           dimrem = elina_dimchange_alloc(0,num_in_pixels)\n",
    "           for i in range(num_in_pixels):\n",
    "               dimrem.contents.dim[i] = i\n",
    "           elina_abstract0_remove_dimensions(man, True, element, dimrem)\n",
    "           elina_dimchange_free(dimrem)\n",
    "           # handle ReLU layer \n",
    "           if(nn.layertypes[layerno]=='ReLU'):\n",
    "              element = relu_box_layerwise(man,True,element,0, num_out_pixels)\n",
    "           nn.ffn_counter+=1 \n",
    "\n",
    "        else:\n",
    "           print(' net type not supported')\n",
    "   \n",
    "    dims = elina_abstract0_dimension(man,element)\n",
    "    output_size = dims.intdim + dims.realdim\n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "\n",
    "           \n",
    "    # if epsilon is zero, try to classify else verify robustness \n",
    "    \n",
    "    verified_flag = True\n",
    "    predicted_label = 0\n",
    "    if(LB_N0[0]==UB_N0[0]):\n",
    "        for i in range(output_size):\n",
    "            inf = bounds[i].contents.inf.contents.val.dbl\n",
    "            flag = True\n",
    "            for j in range(output_size):\n",
    "                if(j!=i):\n",
    "                   sup = bounds[j].contents.sup.contents.val.dbl\n",
    "                   if(inf<=sup):\n",
    "                      flag = False\n",
    "                      break\n",
    "            if(flag):\n",
    "                predicted_label = i\n",
    "                break    \n",
    "    else:\n",
    "        inf = bounds[label].contents.inf.contents.val.dbl\n",
    "        for j in range(output_size):\n",
    "            if(j!=label):\n",
    "                sup = bounds[j].contents.sup.contents.val.dbl\n",
    "                if(inf<=sup):\n",
    "                    predicted_label = label\n",
    "                    verified_flag = False\n",
    "                    break\n",
    "\n",
    "    elina_interval_array_free(bounds,output_size)\n",
    "    elina_abstract0_free(man,element)\n",
    "    elina_manager_free(man)        \n",
    "    return predicted_label, verified_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define operations on abstract domain using linear approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hidden_constraint(model, layerno, z, z_hat, weights, biases):\n",
    "    \"\"\"\n",
    "    This function computes “which side” of the ReLU the pre-ReLU activations lies on.\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - layerno: layer number from which z_hat belong\n",
    "        - z: gurobi variables for hidden layer input\n",
    "        - z_hat: gurobi variables for hidden layer output\n",
    "        - weights: weights for the hidden layer\n",
    "        - bias: bias in the hidden layer\n",
    "    OUTPUT:\n",
    "        - model: gurobi model with new hidden constrains\n",
    "   \"\"\"\n",
    "    # Sanity check!\n",
    "    assert len(z) == weights.shape[1]\n",
    "    assert len(z_hat) == weights.shape[0]\n",
    "    \n",
    "    # add constraint to model\n",
    "    for i_out in range(len(z_hat)):\n",
    "        constr = LinExpr() + np.asscalar(biases[i_out])\n",
    "        for s in range(len(z)):\n",
    "            constr += z[s] * np.asscalar(weights[i_out, s])\n",
    "\n",
    "        model.addConstr(z_hat[i_out] == constr, \\\n",
    "                    name=\"hidden_constr_\" + str(layerno) + \"_\" + str(i_out))\n",
    "    \n",
    "    model.update()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relu_activation_constraint(model, layerno, z_hat, z, LB, UB):\n",
    "    \"\"\"\n",
    "    This function computes “which side” of the ReLU the pre-ReLU activations lies on.\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - layerno: layer number from which z_hat belong\n",
    "        - z_hat: gurobi variables for pre-relu input\n",
    "        - z: gurobi variables for relu output\n",
    "        - LB: lower bound of inputs to a relu layer\n",
    "        - UB: upper bound of inputs to a relu layer\n",
    "    OUTPUT:\n",
    "        - model: gurobi model with new ReLU constrains\n",
    "   \"\"\"\n",
    "    # Sanity check!\n",
    "    assert len(z) == len(UB)\n",
    "    \n",
    "    # iterate over each pre-relu neuron activation\n",
    "    for j in range(len(UB)):\n",
    "        u = np.asscalar(UB[j])\n",
    "        l = np.asscalar(LB[j])\n",
    "\n",
    "        if u <= 0:\n",
    "            model.addConstr(z[j] == 0, \\\n",
    "                        name=\"relu_constr_deac_\" + str(layerno) + \"_\" + str(j))\n",
    "        elif l > 0:\n",
    "            model.addConstr(z[j] == z_hat[j], \\\n",
    "                        name=\"relu_constr_deac_\" + str(layerno) + \"_\" + str(j))\n",
    "        else:\n",
    "            alpha = u/(u - l)\n",
    "            model.addConstr(z[j] >= 0 , \\\n",
    "                         name=\"relu_const_ambi_pos_\" + str(layerno) + \"_\" + str(j))\n",
    "            model.addConstr(z[j] >= z_hat[j], \\\n",
    "                         name=\"relu_const_ambi_hid_\" + str(layerno) + \"_\" + str(j))\n",
    "            model.addConstr(z[j] <= alpha * (z_hat[j] - l), \\\n",
    "                         name=\"relu_const_ambi_lin_\" + str(layerno) + \"_\" + str(j))\n",
    "    model.update()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_linear_solver(model, z_hat, lb_only=False, ub_only=False):\n",
    "    \"\"\"\n",
    "    This function computes lower and upper bound for given objective function and model\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - z_hat: gurobi variable to optimize for\n",
    "    OUTPUT:\n",
    "        - LB: lower bound of variable\n",
    "        - UB: upper bound of variable\n",
    "   \"\"\"\n",
    "    # Sanity\n",
    "    assert not lb_only*ub_only\n",
    "    LB, UB = None, None\n",
    "    \n",
    "    if not ub_only:\n",
    "        # Find Lower Bound\n",
    "        model.setObjective(z_hat, GRB.MINIMIZE)\n",
    "        model.update()\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status == GRB.Status.OPTIMAL:\n",
    "            LB = model.objVal\n",
    "        else:\n",
    "            raise(RuntimeError('[Min] Error. Not Able to retrieve bound. Gurobi Model. Not Optimal.'))\n",
    "\n",
    "        # reset model \n",
    "        model.reset()\n",
    "    \n",
    "    if not lb_only:\n",
    "        # Find Upper Bound\n",
    "        model.setObjective(z_hat, GRB.MAXIMIZE)\n",
    "        model.update()\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status == GRB.Status.OPTIMAL:\n",
    "            UB = model.objVal\n",
    "        else:\n",
    "            raise(RuntimeError('[Max] Error. Not Able to retrieve bound. Gurobi Model. Not Optimal.'))\n",
    "\n",
    "        # reset model \n",
    "        model.reset()\n",
    "    \n",
    "    return LB, UB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define operations on abstract domain using Box approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relu_bounds_using_box(man, input_LB, input_UB, num_in_pixels):\n",
    "    '''\n",
    "    This function calculates the bounds of a ReLU operation. \n",
    "    INPUT:\n",
    "        - man: pointer to elina manager\n",
    "        - input_LB: lower bound of the inputs to the ReLU\n",
    "        - input_UB: upper bound of the inputs to the ReLU\n",
    "        - num_in_pixels: number of inputs to ReLU\n",
    "    \n",
    "    OUTPUT:\n",
    "        - output_LB: lower bound of the outputs from ReLU layer\n",
    "        - output_UB: upper bound of the outputs from ReLU layer\n",
    "        - num_out_pixels: number of outputs of ReLI layer\n",
    "    '''\n",
    "    itv = elina_interval_array_alloc(num_in_pixels)\n",
    "\n",
    "    ## Populate the interval\n",
    "    for i in range(num_in_pixels):\n",
    "        elina_interval_set_double(itv[i], input_LB[i], input_UB[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_in_pixels, itv)\n",
    "    elina_interval_array_free(itv, num_in_pixels)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Handle ReLU Layer\n",
    "    # ------------------------------------------------------------------\n",
    "    num_out_pixels = num_in_pixels\n",
    "    \n",
    "    element = relu_box_layerwise(man, True, element,0, num_in_pixels)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    output_LB = np.zeros((num_out_pixels, 1), float)\n",
    "    output_UB = np.zeros((num_out_pixels, 1), float)\n",
    "    for j in range(num_out_pixels):\n",
    "        output_LB[j] = bounds[j].contents.inf.contents.val.dbl\n",
    "        output_UB[j] = bounds[j].contents.sup.contents.val.dbl\n",
    "    \n",
    "    # free out the memory allocations\n",
    "    elina_interval_array_free(bounds, num_out_pixels)\n",
    "    elina_abstract0_free(man, element)\n",
    "    \n",
    "    return output_LB, output_UB, num_out_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_bounds_using_box(man, weights, biases, input_LB, input_UB, num_in_pixels, verbose=False):\n",
    "    '''\n",
    "    This function calculates the bounds of a ReLU operation followed by a hidden layer. \n",
    "    INPUT:\n",
    "        - man: pointer to elina manager\n",
    "        - weights: weights of the hidden layer\n",
    "        - biases: biases of the hidden layer\n",
    "        - input_LB: lower bound of the inputs to the hidden layer\n",
    "        - input_UB: upper bound of the inputs to the hidden layer\n",
    "        - num_in_pixels: number of inputs to the input layer\n",
    "    \n",
    "    OUTPUT:\n",
    "        - output_LB: lower bound of the outputs from hidden layer\n",
    "        - output_UB: upper bound of the outputs from hidden layer\n",
    "        - num_out_pixels: number of outputs of hidden layer\n",
    "    '''\n",
    "    itv = elina_interval_array_alloc(num_in_pixels)\n",
    "\n",
    "    ## Populate the interval\n",
    "    for i in range(num_in_pixels):\n",
    "        elina_interval_set_double(itv[i], input_LB[i], input_UB[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_in_pixels, itv)\n",
    "    elina_interval_array_free(itv, num_in_pixels)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Handle Affine Layer\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # calculate number of outputs\n",
    "    num_out_pixels = len(weights)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[Network] Input pixels: \" + str(num_in_pixels))\n",
    "        print(\"[Network] Shape of weights: \" + str(np.shape(weights)))\n",
    "        print(\"[Network] Shape of biases: \" + str(np.shape(biases)))\n",
    "        print(\"[Network] Out pixels: \" + str(num_out_pixels))\n",
    "\n",
    "    # Create number of neurons in the layer and populate it\n",
    "    # with the number of inputs to each neuron in the layer\n",
    "    dimadd = elina_dimchange_alloc(0, num_out_pixels)    \n",
    "    for i in range(num_out_pixels):\n",
    "        dimadd.contents.dim[i] = num_in_pixels\n",
    "\n",
    "    # Add dimensions to an ElinaAbstract0 pointer i.e. element\n",
    "    elina_abstract0_add_dimensions(man, True, element, dimadd, False)\n",
    "    elina_dimchange_free(dimadd)\n",
    "\n",
    "    # Create the linear expression associated each neuron\n",
    "    var = num_in_pixels\n",
    "    for i in range(num_out_pixels):\n",
    "        tdim = ElinaDim(var)\n",
    "        linexpr0 = generate_linexpr0(weights[i], biases[i], num_in_pixels)\n",
    "        # Parallel assignment of several dimensions of an ElinaAbstract0 by using an ElinaLinexpr0Array\n",
    "        element = elina_abstract0_assign_linexpr_array(man, True, element, tdim, linexpr0, 1, None)\n",
    "        var += 1\n",
    "\n",
    "    # Pointer to which semantics we want to follow.\n",
    "    dimrem = elina_dimchange_alloc(0, num_in_pixels)\n",
    "    for i in range(num_in_pixels):\n",
    "        dimrem.contents.dim[i] = i\n",
    "        \n",
    "    # Remove dimensions from an ElinaAbstract0\n",
    "    elina_abstract0_remove_dimensions(man, True, element, dimrem)\n",
    "    elina_dimchange_free(dimrem)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    output_LB = np.zeros((num_out_pixels, 1), float)\n",
    "    output_UB = np.zeros((num_out_pixels, 1), float)\n",
    "    for j in range(num_out_pixels):\n",
    "        output_LB[j] = bounds[j].contents.inf.contents.val.dbl\n",
    "        output_UB[j] = bounds[j].contents.sup.contents.val.dbl    \n",
    "    \n",
    "    # free out the memory allocations\n",
    "    elina_interval_array_free(bounds, num_out_pixels)\n",
    "    elina_abstract0_free(man, element)\n",
    "    \n",
    "    return output_LB, output_UB, num_out_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to verify the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_network(LB_N0, UB_N0, LB_NN, UB_NN, label, num_input_pixels = 784, num_out_pixels = 10):\n",
    "    '''\n",
    "    This function verifies the network given the bounds of the input layer and the final layer of the network.\n",
    "    INPUT:\n",
    "        - LB_N0: lower bounds of the preturbed input image\n",
    "        - UB_N0: unpper bounds of the preturbed input image\n",
    "        - LB_NN: lower bounds of the final layer of neural network\n",
    "        - UB_NN: upper bounds of the final layer of neural network\n",
    "        - label: true label of the input image\n",
    "        - num_input_pixels: number of pixels in the input image (for MNIST, default: 784)\n",
    "        - num_out_pixels: number of neurons in the last layer of the network  (for MNIST, default: 10)\n",
    "    \n",
    "    OUTPUT:\n",
    "        - predicted_label: label predicted by the neural network\n",
    "        - verified_flag: boolean variable, true if the network is robust to perturbation\n",
    "    '''\n",
    "    \n",
    "    # if epsilon is zero, try to classify else verify robustness \n",
    "    verified_flag = True\n",
    "    predicted_label = 0\n",
    "    if(LB_N0[0]==UB_N0[0]):\n",
    "        for i in range(num_out_pixels):\n",
    "            inf = LB_NN[i]\n",
    "            flag = True\n",
    "            for j in range(num_out_pixels):\n",
    "                if(j!=i):\n",
    "                    sup = UB_NN[j]\n",
    "                    if(inf<=sup):\n",
    "                        flag = False\n",
    "                        break\n",
    "            if(flag):\n",
    "                predicted_label = i\n",
    "                break    \n",
    "    else:\n",
    "        inf = LB_NN[label]\n",
    "        for j in range(num_out_pixels):\n",
    "            if(j!=label):\n",
    "                sup = UB_NN[j]\n",
    "                if(inf<=sup):\n",
    "                    predicted_label = label\n",
    "                    verified_flag = False\n",
    "                    break\n",
    "\n",
    "    if(verified_flag):\n",
    "        print(\"verified\")\n",
    "    else:\n",
    "        print(\"can not be verified\")  \n",
    "        \n",
    "    return predicted_label, verified_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to perform different analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform box analysis for all layers in the neural network nn\n",
    "def perform_box_analysis(nn, LB_N0, UB_N0, verbose = False):\n",
    "    # create a list to store the bounds found through box approximation\n",
    "    LB_hidden_box_list = []\n",
    "    UB_hidden_box_list = []\n",
    "\n",
    "    # create manager for Elina\n",
    "    man = elina_box_manager_alloc()\n",
    "\n",
    "    # initialize variables for the network iteration\n",
    "    numlayer = nn.numlayer \n",
    "    nn.ffn_counter = 0\n",
    "\n",
    "    # for input image\n",
    "    input_LB = LB_N0.copy()\n",
    "    input_UB = UB_N0.copy()\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input Layer, size: \" + str(len(LB_N0)))\n",
    "        print('---------------')\n",
    "\n",
    "    for layerno in range(numlayer):\n",
    "        if verbose:\n",
    "            print(\"Layer Number: \" + str(layerno + 1))\n",
    "\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            if verbose:\n",
    "                print(\"Layer Type: %s\" % nn.layertypes[layerno])\n",
    "\n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.double)\n",
    "            np.ascontiguousarray(biases, dtype=np.double)\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Handle Affine Layer\n",
    "            # ------------------------------------------------------------------\n",
    "            output_LB, output_UB, num_out_pixels = get_hidden_bounds_using_box(man, weights, biases, input_LB, input_UB, num_in_pixels, verbose)\n",
    "\n",
    "            # Add bounds to the list\n",
    "            LB_hidden_box_list.append(output_LB.copy())\n",
    "            UB_hidden_box_list.append(output_UB.copy())\n",
    "            # Prepare variables for next layer\n",
    "            input_LB = output_LB.copy()\n",
    "            input_UB = output_UB.copy()\n",
    "            num_in_pixels = num_out_pixels\n",
    "            nn.ffn_counter += 1 \n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Handle ReLU Layer\n",
    "            # ------------------------------------------------------------------\n",
    "            if(nn.layertypes[layerno] == \"ReLU\"):\n",
    "                output_LB, output_UB, num_out_pixels = get_relu_bounds_using_box(man, input_LB, input_UB, num_in_pixels)\n",
    "\n",
    "            # Prepare variables for next layer\n",
    "            input_LB = output_LB.copy()\n",
    "            input_UB = output_UB.copy()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"[OUTPUT] Bounds: \")\n",
    "                output_LB, output_UB  = output_LB.squeeze(), output_UB.squeeze()\n",
    "                pprint(np.stack((output_LB, output_UB), axis=1))\n",
    "            \n",
    "            if verbose:\n",
    "                print('---------------')\n",
    "\n",
    "        else:\n",
    "            print(' net type not supported')\n",
    "    if verbose:\n",
    "        print(\"Output Layer, size: \" + str(len(output_LB)))\n",
    "\n",
    "    elina_manager_free(man)\n",
    "    \n",
    "    # for last layer of the netowork is ReLU\n",
    "    LB_NN = LB_hidden_box_list[-1].copy()\n",
    "    UB_NN = UB_hidden_box_list[-1].copy()\n",
    "\n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(LB_hidden_box_list[-1])\n",
    "        for i in range(num_out):\n",
    "            if LB_hidden_box_list[-1][i] < 0 :\n",
    "                LB_NN[i] = 0 \n",
    "            if UB_hidden_box_list[-1][i] < 0 :\n",
    "                UB_NN[i] = 0 \n",
    "            \n",
    "    return LB_hidden_box_list, UB_hidden_box_list, LB_NN.squeeze(), UB_NN.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_over_box_approximation(nn, LB_N0, UB_N0, LB_hidden_box_list, UB_hidden_box_list, verbose = False):\n",
    "    # initialize variables for the network iteration\n",
    "    numlayer = nn.numlayer \n",
    "    nn.ffn_counter = 0\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    \n",
    "    m = get_model()\n",
    "    \n",
    "    # We follow the following notations:\n",
    "    # z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "    # z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "    # for input layer:\n",
    "    # Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "    num_in_pixels = len(LB_N0)\n",
    "    img_vars = m.addVars(num_in_pixels, lb=LB_N0, ub=UB_N0, \\\n",
    "                         vtype=GRB.CONTINUOUS, name=\"input_layer\")\n",
    "    \n",
    "    # for output of each ReLU\n",
    "    z = []\n",
    "    z.append(img_vars)\n",
    "    # for output of each hidden layer\n",
    "    z_hat = []\n",
    "\n",
    "    # Create variables for all layers and append to the list \n",
    "    m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "    m.update()\n",
    "    \n",
    "    if verbose: \n",
    "        # Sanity check!\n",
    "        # Size of z should be number of relu activation layers + 1 (for input)\n",
    "        print(\"Number of relu layers: {0}\".format(len(z)))\n",
    "        # Size of z_hat should be number of hidden layers\n",
    "        print(\"Number of hidden layers: {0}\".format(len(z_hat)))\n",
    "        print(\"Size of last hidden layer: {0}\".format(len(z_hat[-1])))\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "    nn.ffn_counter = 0\n",
    "\n",
    "    # Adding weights constraints for k layers\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.float)\n",
    "            np.ascontiguousarray(biases, dtype=np.float)\n",
    "\n",
    "            # add affine constraint\n",
    "            add_hidden_constraint(m, layerno, z[layerno], z_hat[layerno], weights, biases)\n",
    "        \n",
    "            # update counter for next iteration\n",
    "            nn.ffn_counter += 1\n",
    "        else:\n",
    "            raise(\"Not a valid layer!\")\n",
    "        \n",
    "    m.update()\n",
    "\n",
    "    # Adding relu constraints for (k-1) layers. The loop starts from z_2 since z_1 is input\n",
    "    for i in range(1, numlayer):        \n",
    "        # add relu constraint\n",
    "        if (nn.layertypes[layerno] in [\"ReLU\"]):\n",
    "            add_relu_activation_constraint(m, layerno, z_hat[i-1], z[i], LB_hidden_box_list[i-1], UB_hidden_box_list[i-1])\n",
    "\n",
    "    m.update()\n",
    "\n",
    "    # storing upper and lower bounds for last layer\n",
    "    UB = np.zeros_like(nn.biases[-1])\n",
    "    LB = np.zeros_like(nn.biases[-1])\n",
    "    numlayer = nn.numlayer \n",
    "\n",
    "    # Solving for each neuron in the output layer to collect bounds\n",
    "    # i.e. z_hat_{-1} where -1 denotes the last array in list\n",
    "    for i_out in range(len(UB)): \n",
    "        LB[i_out], UB[i_out] = call_linear_solver(m, z_hat[-1][i_out])\n",
    "        \n",
    "    # for last layer of the netowork is ReLU\n",
    "    LB_NN = LB\n",
    "    UB_NN = UB\n",
    "    \n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(UB_NN)\n",
    "\n",
    "        for i in range(num_out):\n",
    "            if LB[i] < 0 :\n",
    "                LB_NN[i] = 0 \n",
    "            if UB[i] < 0 :\n",
    "                UB_NN[i] = 0 \n",
    "                \n",
    "    return LB_NN.squeeze(), UB_NN.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(single_thread=False):\n",
    "    \"\"\"\n",
    "    Get Gurobi model\n",
    "    \"\"\"\n",
    "    m = Model(\"LP\")\n",
    "    m.setParam(\"outputflag\", False)\n",
    "\n",
    "    # disable parallel Gurobi solver\n",
    "    m.setParam(\"Method\", 1)  # dual simplex\n",
    "    if single_thread:\n",
    "        m.setParam(\"Threads\", 1) # only 1 thread\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list, verbose=True):\n",
    "    \"\"\"\n",
    "    Add and create all variables of neural network to gurobi model.\n",
    "    INPUT:\n",
    "        - m: Gurobi model\n",
    "        - numlayer: Number of Layers\n",
    "        - LB_N0: Lower Bound of perturbed image input\n",
    "        - UB_N0: Upper Bound of perturbed image input\n",
    "        - UB_hidden_box_list: List of upper Bounds from box approximation (needed to set upper bound of ReLU outputs)\n",
    "    OUTPUT:\n",
    "        - m: Gurobi model with newly added variables\n",
    "        - z: List of Gurobi variables corresponding to pre-ReLU Layer (hidden)\n",
    "        - z_hat: List of Gurobi variables corresponding to post-ReLU Layer\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # for output of each ReLU\n",
    "    z = []\n",
    "    # for output of each hidden layer\n",
    "    z_hat = []\n",
    "    \n",
    "    # Create variables of input image\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    img_vars = m.addVars(num_in_pixels, lb=LB_N0, ub=UB_N0, \\\n",
    "                 vtype=GRB.CONTINUOUS, name=\"input_layer\")\n",
    "    z.append(img_vars)\n",
    "    \n",
    "    # Create variables for all layers and append to the list \n",
    "    for i in range(numlayer):\n",
    "        # for layers before the final layer, z_hat and z exists\n",
    "        if i < (numlayer - 1):\n",
    "\n",
    "            UB_relu = UB_hidden_box_list[i].squeeze().copy()\n",
    "            for j in range(len(UB_hidden_box_list[i])):\n",
    "                bound = UB_hidden_box_list[i][j]\n",
    "                UB_relu[j] = max(0, bound)\n",
    "            UB_relu.squeeze() \n",
    "\n",
    "            # middle layer, has both z and z hat\n",
    "            z_hat_hidden = m.addVars(len(UB_hidden_box_list[i]), lb=-np.inf, ub=np.inf, \\\n",
    "                                     vtype=GRB.CONTINUOUS, name=\"hidden_layer_\" + str(i))\n",
    "            z_relu = m.addVars(len(UB_hidden_box_list[i]), lb=0.0, ub = UB_relu,\\\n",
    "                               vtype=GRB.CONTINUOUS, name=\"relu_layer_\" + str(i))\n",
    "            # append to the list\n",
    "            z_hat.append(z_hat_hidden)\n",
    "            z.append(z_relu)\n",
    "        # for last layer, only z_hat exists\n",
    "        else: \n",
    "            z_hat_hidden = m.addVars(len(UB_hidden_box_list[i]), lb=-np.inf, ub=np.inf, \\\n",
    "                                     vtype=GRB.CONTINUOUS, name=\"output_layer\") \n",
    "            # append to the list\n",
    "            z_hat.append(z_hat_hidden)\n",
    "\n",
    "    m.update()\n",
    "    \n",
    "    if verbose:\n",
    "        # Sanity check!\n",
    "        # Size of z should be number of relu activation layers + 1 (for input)\n",
    "        print(\"Number of relu layers: {0}\".format(len(z)))\n",
    "        # Size of z_hat should be number of hidden layers\n",
    "        print(\"Number of hidden layers: {0}\".format(len(z_hat)))\n",
    "        print(\"Size of last hidden layer: {0}\".format(len(z_hat[-1])))\n",
    "    \n",
    "    return m, z, z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(nn, norm=1, skip_first_layer=True, skip_last_layer=True):\n",
    "    numlayer = nn.numlayer \n",
    "    norms = np.zeros((nn.weights[1].shape[0], numlayer-2)) # TODO use list or similar if first and last layer are also used\n",
    "    rank_local_idxs = np.zeros((nn.weights[1].shape[0], numlayer-2))\n",
    "    n_neurons = rank_local_idxs.shape[0] * rank_local_idxs.shape[1]\n",
    "    \n",
    "    for layerno in range(numlayer):\n",
    "        if skip_first_layer and layerno == 0:\n",
    "            #TODO treat first layer\n",
    "            continue\n",
    "        if skip_last_layer and layerno == numlayer-1:\n",
    "            continue\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            weights = nn.weights[layerno]\n",
    "            biases = nn.biases[layerno]\n",
    "#             norms[:, layerno-1] = np.linalg.norm(weights, ord=norm, axis=1) + biases\n",
    "            norms[:, layerno-1] = np.linalg.norm(weights, ord=norm, axis=0)\n",
    "\n",
    "        else:\n",
    "            print(' net type not supported')\n",
    "    \n",
    "    # Global rank\n",
    "    temp = np.argsort(-norms, axis=None)\n",
    "    rank_idxs = np.empty_like(temp)\n",
    "    rank_idxs[temp] = np.arange(len(temp))\n",
    "    rank_idxs = rank_idxs.reshape(nn.weights[1].shape[0], int(len(rank_idxs)/ nn.weights[1].shape[0])) /n_neurons\n",
    "    \n",
    "    # Local rank\n",
    "    temp = np.argsort(-norms, axis=0)\n",
    "    rank_local_idxs = np.empty_like(temp)\n",
    "    for layerno in range(numlayer-2):\n",
    "        rank_local_idxs[temp[:,layerno],layerno] = np.arange(norms.shape[0])/rank_local_idxs.shape[0]\n",
    "    \n",
    "    for layerno in range(numlayer):\n",
    "        if skip_first_layer and layerno == 0:\n",
    "            continue\n",
    "        if skip_last_layer and layerno == numlayer-1:\n",
    "            continue\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            nn.rank[layerno] = {}\n",
    "            nn.rank[layerno]['global'] = rank_idxs[:, layerno-1] # TODO Change minus 1 to flexibe offset\n",
    "            nn.rank[layerno]['local'] = rank_local_idxs[:, layerno-1] # TODO Change minus 1 to flexibe offset\n",
    "#             nn.use_LP[layerno] = nn.rank[layerno]  < rank_threshold\n",
    "        else:\n",
    "            print(' net type not supported')\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list, LB_hidden_box_list, UB_hidden_box_list, \n",
    "                             true_label, global_rank_threshold=[1.0, 0.0], local_rank_threshold=[0.0, 0.0], \n",
    "                             tightness_threshold=[0.0, 0.0] ,verbose=True):\n",
    "    \"\"\"\n",
    "    Get final bounds using linear programming layerwise. If lp_freq > 1 linear bounds are only calculated every \n",
    "    lp_freq'th layer.\n",
    "    INPUT:\n",
    "        - m: Gurobi model\n",
    "        - z: List of Gurobi variables corresponding to pre-ReLU Layer (hidden)\n",
    "        - z_hat: List of Gurobi variables corresponding to post-ReLU Laye\n",
    "        - nn: Neural Network as defined in initial code (contains layertypes, weights, etc.)\n",
    "        - numlayer: Number of Layers\n",
    "        - LB_N0: Lower Bound of perturbed image input\n",
    "        - UB_N0: Upper Bound of perturbed image input\n",
    "        - lp_list: Layerno to start solvingbounds by LP. \n",
    "        - prob: probability to select a neuron\n",
    "        - LB_hidden_box_list: List of upper Bounds from box approximation\n",
    "        - UB_hidden_box_list: List of upper Bounds from box approximation\n",
    "        - global_rank_threshold: List of global rank threshold where LP is used. First element corresponds \n",
    "                                 to threshold of layers in lp_list, second element to other layers.\n",
    "                                 Therefore, if layer is in lp_list, use LP if rank of Neuron is below \n",
    "                                 global_rank_threshold[0], default is use LP on ALL neurons of lp_list layer.\n",
    "                                 If layer is not in lp_list, use LP if rank of Neuron is below \n",
    "                                 global_rank_threshold[1], default do NOT use LP on neurons.\n",
    "        - local_rank_threshold:  List of local rank threshold where LP is used. First element corresponds \n",
    "                                 to threshold of layers in lp_list, second element to other layers.\n",
    "                                 Therefore, if layer is in lp_list, use LP if rank of Neuron is below \n",
    "                                 local_rank_threshold[0], default is use LP on ALL neurons of lp_list layer.\n",
    "                                 If layer is not in lp_list, use LP if rank of Neuron is below \n",
    "                                 local_rank_threshold[1], default do NOT use LP on neurons.\n",
    "                                 \n",
    "    OUTPUT:\n",
    "        - LB_NN: Lower bounds of neural network output\n",
    "        - UB_NN: Upper bounds of neural network output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity check\n",
    "    assert LB_hidden_box_list is not None \n",
    "    assert UB_hidden_box_list is not None\n",
    "    assert len(global_rank_threshold)==2\n",
    "    assert len(local_rank_threshold)==2\n",
    "    assert all( v <=1.0 or v>=0 for v in global_rank_threshold)\n",
    "    assert all( v <=1.0 or v>=0 for v in local_rank_threshold)\n",
    "\n",
    "    # create manager for Elina\n",
    "    man = elina_box_manager_alloc()\n",
    "    \n",
    "    # create gurobi model\n",
    "    m = get_model()\n",
    "    # create all gurobi variables for the network\n",
    "    m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "    \n",
    "    # initialize counter\n",
    "    nn.ffn_counter = 0\n",
    "    \n",
    "    # Compute rank\n",
    "    compute_rank(nn, norm=1, skip_first_layer=True, skip_last_layer=True)\n",
    "    \n",
    "    # Init statistics\n",
    "    stats = {'time': [],\n",
    "        'LB_hat': [],\n",
    "        'UB_hat': [],\n",
    "        'use_LP': [],\n",
    "        'local_rank': [],\n",
    "        'global_rank': [],\n",
    "        'margin': [],\n",
    "        'margin_per_neuron': [],\n",
    "        'margin_per_time': [],\n",
    "        'tightness_hat': [],\n",
    "        'min_tightness_hat': [],\n",
    "        'max_tightness_hat': [],\n",
    "        'median_tightness_hat': [],\n",
    "        }\n",
    "\n",
    "    # Adding weights constraints for k layers\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            t1 = time.time()\n",
    "                        \n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.float)\n",
    "            np.ascontiguousarray(biases, dtype=np.float)\n",
    "\n",
    "            # output shape of the layer\n",
    "            n_in = weights.shape[1]\n",
    "            n_out = weights.shape[0]\n",
    "\n",
    "            # create variables to store bounds of hidden layer\n",
    "            LB_hat = np.zeros(n_out, float)\n",
    "            UB_hat = np.zeros(n_out, float)\n",
    "\n",
    "            # add affine constraint\n",
    "            add_hidden_constraint(m, layerno, z[layerno], z_hat[layerno], weights, biases)\n",
    "            \n",
    "            ##########################################\n",
    "            # First layer: Use original BOX bounds   #\n",
    "            ##########################################\n",
    "            if layerno < lp_list[0] or layerno == 0:\n",
    "                use_LP = np.zeros(n_out, dtype=np.bool)\n",
    "                LB_hat, UB_hat = LB_hidden_box_list[layerno].copy() , UB_hidden_box_list[layerno].copy()\n",
    "            \n",
    "            #########################################\n",
    "            # Last Layer: Use Linear Programming    #\n",
    "            #########################################\n",
    "            elif layerno == numlayer - 1:\n",
    "                use_LP = np.ones(n_out, dtype=np.bool)\n",
    "                \n",
    "                # LB: Get lower bound of correct label\n",
    "                LB_hat[true_label], _ = call_linear_solver(m, z_hat[layerno][true_label], lb_only=True)\n",
    "                \n",
    "                # UB: Get upper bound of all other labels    \n",
    "                for i_out in range(n_out):\n",
    "                    if i_out == true_label:\n",
    "                        continue\n",
    "                        \n",
    "                    _, UB_hat[i_out] = call_linear_solver(m, z_hat[layerno][i_out], ub_only=True)\n",
    "                \n",
    "            ######################################################\n",
    "            # All layers inbetween: Decide if we use LP or BOX   #\n",
    "            ######################################################\n",
    "            else:\n",
    "                \n",
    "                # Calculate box to get tigthness\n",
    "                LB, UB, n_out = get_relu_bounds_using_box(man, LB_hat_prev, UB_hat_prev, n_in)\n",
    "                LB_hat, UB_hat, n_out = get_hidden_bounds_using_box(man, weights, biases, \n",
    "                                                                LB, UB, n_out, verbose)\n",
    "                \n",
    "                # Get Tighness Rank\n",
    "                tightness_box = UB_hat.squeeze() - LB_hat.squeeze()\n",
    "                temp = np.argsort(-tightness_box, axis=0)\n",
    "                tightness_box_rank = np.empty_like(temp)\n",
    "                tightness_box_rank[temp] = np.arange(len(temp))\n",
    "                tightness_box_rank = tightness_box_rank/n_out\n",
    "                # LP\n",
    "                # If the layer is in the list of layer use LP on ALL neurons\n",
    "                if (layerno in lp_list):\n",
    "                    # Use LP if the weight of the rank is below the specified weights.\n",
    "                    # use_LP = np.ones(n_out, dtype=np.bool)\n",
    "                    use_LP = (nn.rank[layerno]['global'] < global_rank_threshold[0]) | \\\n",
    "                                                 (nn.rank[layerno]['local'] < local_rank_threshold[0])\n",
    "                    use_LP = (use_LP) | (tightness_box_rank < tightness_threshold[0])\n",
    "                    \n",
    "                # LP\n",
    "                # If the layer is NOT in list of layer use LP only on high ranking neurons\n",
    "                else:\n",
    "                    # Use LP if the rank of the weight is below the specified threshold.\n",
    "                    use_LP = (nn.rank[layerno]['global'] < global_rank_threshold[1]) | \\\n",
    "                                                 (nn.rank[layerno]['local'] < local_rank_threshold[1])\n",
    "                    use_LP = (use_LP) | (tightness_box_rank < tightness_threshold[1])\n",
    "                    \n",
    "#                 # Combine use_LP from nn (previous iter) and new use_LP\n",
    "#                 use_LP = use_LP and not nn.use_LP[layerno]\n",
    "                \n",
    "                # Calculate BOX bounds if not using LP on ALL neurons\n",
    "                if not use_LP.all():\n",
    "                    # Get Box bounds and overwrite if we decide to use LP\n",
    "                    LB, UB, n_out = get_relu_bounds_using_box(man, LB_hat_prev, UB_hat_prev, n_in)\n",
    "                    LB_hat, UB_hat, n_out = get_hidden_bounds_using_box(man, weights, biases, \n",
    "                                                                LB, UB, n_out, verbose)\n",
    "                \n",
    "                for i_out in range(n_out):\n",
    "                    if use_LP[i_out]:\n",
    "                        LB_hat[i_out], UB_hat[i_out] = call_linear_solver(m, z_hat[layerno][i_out])\n",
    "                    \n",
    "            \n",
    "            # add relu constraint\n",
    "            if layerno < (numlayer - 1) and nn.layertypes[layerno] in [\"ReLU\"]:\n",
    "                add_relu_activation_constraint(m, layerno, z_hat[layerno], z[layerno + 1], LB_hat, UB_hat)\n",
    "            \n",
    "            # preparation for next iteration    \n",
    "            LB_hat_prev, UB_hat_prev = LB_hat.copy(), UB_hat.copy()\n",
    "                \n",
    "            m.update()\n",
    "\n",
    "            # update counter for next iteration\n",
    "            nn.ffn_counter += 1\n",
    "            # Save where we used LP\n",
    "            nn.use_LP[layerno] = use_LP\n",
    "            nn.LB_hat[layerno] = LB_hat\n",
    "            nn.UB_hat[layerno] = UB_hat\n",
    "            \n",
    "            \n",
    "            # Save stats\n",
    "            t2 = time.time()\n",
    "            \n",
    "            stats['time'].append(t2 - t1)\n",
    "            stats['LB_hat'].append(LB_hat.squeeze())\n",
    "            stats['UB_hat'].append(UB_hat.squeeze())\n",
    "            try:\n",
    "                stats['global_rank'].append(nn.rank[layerno]['global'])\n",
    "            except Exception:\n",
    "                nan_array = np.empty(stats['LB_hat'][-1].shape)\n",
    "                nan_array[:] = np.nan\n",
    "                stats['global_rank'].append(nan_array)  \n",
    "            try:\n",
    "                stats['local_rank'].append(nn.rank[layerno]['local'])\n",
    "            except Exception:\n",
    "                nan_array = np.empty(stats['LB_hat'][-1].shape)\n",
    "                nan_array[:] = np.nan\n",
    "                stats['local_rank'].append(nan_array)   \n",
    "            stats['use_LP'].append(use_LP)\n",
    "            tightness = stats['UB_hat'][-1] - stats['LB_hat'][-1]\n",
    "            stats['tightness_hat'].append(tightness)\n",
    "            stats['median_tightness_hat'].append(np.median(tightness))\n",
    "            stats['min_tightness_hat'].append(min(tightness))\n",
    "            stats['max_tightness_hat'].append(max(tightness))\n",
    "\n",
    "            if layerno == (numlayer - 1):\n",
    "                stats['margin'].append(LB_hat[true_label] - max(UB_hat))\n",
    "                n_use_lp = sum([sum(lp) for lp in stats['use_LP']])\n",
    "                tot_time = sum(stats['time'])\n",
    "                stats['margin_per_neuron'].append( stats['margin'][-1]/n_use_lp )\n",
    "                stats['margin_per_time'].append(stats['margin'][-1]/tot_time)\n",
    "\n",
    "\n",
    "            \n",
    "            if verbose:\n",
    "                decimals=2\n",
    "                print(\"--------------------------\")\n",
    "                print(\"Layerno: %d\" %layerno)\n",
    "                print(\"Time %3f\" % (t2 - t1))\n",
    "                if stats['use_LP'][-1].any():\n",
    "                    print(\"Time per LP neuron %3f\" % ((t2 - t1)/ sum(stats['use_LP'][-1])))\n",
    "                    print(\"LP used on %d neurons.\" % sum(stats['use_LP'][-1]))\n",
    "                print(\"Median global rank: %3f\" % np.median(stats['global_rank'][-1]))\n",
    "                print(\"Median tigthness of hat bounds: %3f \\n\" % stats['median_tightness_hat'][-1])\n",
    "                print(\"Min tigthness of hat bounds: %3f \\n\" % stats['min_tightness_hat'][-1])\n",
    "                print(\"Max tigthness of hat bounds: %3f \\n\" % stats['max_tightness_hat'][-1])\n",
    "                print(\"[LB_hat | UB_hat | global_rank | local_rank | use_LP]\")\n",
    "                pprint(np.stack([LB_hat.squeeze(), UB_hat.squeeze(),\n",
    "                                stats['global_rank'][-1], stats['local_rank'][-1], stats['use_LP'][-1]], \n",
    "                                axis = 1).round(decimals=decimals))\n",
    "                print(\"--------------------------\\n\")\n",
    "\n",
    "                if layerno == (numlayer - 1):\n",
    "                    print(\"----------SUMMARY-----------\\n\")\n",
    "                    print(\"Verification Margin (more positive better): %3f \\n\" % stats['margin'][-1])\n",
    "                    print(\"Margin per LP neuron (more positve is better): %6f\\n\" % stats['margin_per_neuron'][-1])\n",
    "                    print(\"Margin per second (more positve is better): %10f\\n\" % stats['margin_per_time'][-1])\n",
    "                    print(\"----------END SUMMARY--------\\n\")\n",
    "\n",
    "        else:\n",
    "            raise(\"Not a valid layer!\")\n",
    "    \n",
    "    # Set bounds of last performed layer to output\n",
    "    LB_NN = LB_hat\n",
    "    UB_NN = UB_hat\n",
    "    # If last Layer is RELU change last lower and upper bounds accordingly.\n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(UB_hat)\n",
    "        for i in range(num_out):\n",
    "            if LB_hat[i] < 0 :\n",
    "                LB_NN[i] = 0 \n",
    "            if UB_hat[i] < 0 :\n",
    "                UB_NN[i] = 0 \n",
    "           \n",
    "    return LB_NN, UB_NN, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the problem variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_relu_3_10.txt    mnist_relu_6_100.txt  mnist_relu_9_100.txt\r\n",
      "mnist_relu_3_20.txt    mnist_relu_6_200.txt  mnist_relu_9_200.txt\r\n",
      "mnist_relu_3_50.txt    mnist_relu_6_20.txt\r\n",
      "mnist_relu_4_1024.txt  mnist_relu_6_50.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/riai2018/mnist_nets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get perturbed label and Load NN (provided prediction for unperturbed is true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nn(netname, specname, epsilon):\n",
    "    flag_wrong_label = False\n",
    "    \n",
    "    with open(netname, 'r') as netfile:\n",
    "        netstring = netfile.read()\n",
    "    with open(specname, 'r') as specfile:\n",
    "        specstring = specfile.read()\n",
    "    nn = parse_net(netstring)\n",
    "    x0_low, x0_high = parse_spec(specstring)\n",
    "    LB_N0, UB_N0 = get_perturbed_image(x0_low,0)\n",
    "    \n",
    "    label, _ = analyze(nn,LB_N0,UB_N0,0) # Get label of unperturbed image, i.e. eps=0\n",
    "    \n",
    "    print(\"True label: \" + str(label))\n",
    "    if(label == int(x0_low[0])):\n",
    "        LB_N0, UB_N0 = get_perturbed_image(x0_low, epsilon)\n",
    "    else:\n",
    "        print(\"image not correctly classified by the network. expected label \",int(x0_low[0]), \" classified label: \", label)\n",
    "        flag_wrong_label =  True\n",
    "        \n",
    "    return LB_N0, UB_N0, nn, label, flag_wrong_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 1\n"
     ]
    }
   ],
   "source": [
    "netname = '/home/riai2018/mnist_nets/mnist_relu_6_20.txt'\n",
    "specname = '/home/riai2018/mnist_images/img2.txt'\n",
    "epsilon = 0.001\n",
    "LB_N0, UB_N0, nn, label, flag_wrong_label = load_nn(netname, specname, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n"
     ]
    }
   ],
   "source": [
    "numlayer = nn.numlayer \n",
    "\n",
    "for layerno in range(numlayer):\n",
    "    if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "        print(nn.layertypes[layerno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Find naive/spectral bounds using box approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "LB_hidden_box_list, UB_hidden_box_list, LB_NN, UB_NN = perform_box_analysis(nn, LB_N0, UB_N0, verbose = False)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- For Box ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[ 0.        ,  0.        ],\n",
      "       [ 9.47404515, 10.8678384 ],\n",
      "       [ 2.0732196 ,  2.86511957],\n",
      "       [ 0.60576397,  1.06502746],\n",
      "       [ 0.        ,  0.54891204],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 2.66937381,  3.50468927],\n",
      "       [ 2.38386408,  3.36602735],\n",
      "       [ 0.        ,  0.        ]])\n",
      "[VERIFY] Verification status: \n",
      "verified\n",
      "----------------------------\n",
      "Calculation time: 0.10968637466430664 s\n"
     ]
    }
   ],
   "source": [
    "# check verifiability with box!\n",
    "print(\"--------- For Box ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN, UB_NN], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "t = time.time()\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)\n",
    "print(\"----------------------------\")\n",
    "print(\"Calculation time: %s s\"% (t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Find tighter bounds using linear programming over box intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Paper:__ \"Provable defenses against adversarial examples via the convex outer adversarial polytope\"; J. Zico Kolter, Eric Wong [[PDF]](https://machine-learning-and-security.github.io/papers/mlsec17_paper_42.pdf)\n",
    "\n",
    "We follow the following notation: \n",
    "```\n",
    "Consider a k-layer feedforward ReLU-based NN, f_θ : R^|x| → R^|y|. Given equations: \n",
    "        z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}\n",
    "        z_i = max(z_hat_{i-1} , 0),      where i = {2, . . . , k}\n",
    "and, \n",
    "        z_1 = x\n",
    "        f_θ(x) ≡ z_hat_{k}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Number of relu layers: 6\n",
      "Number of hidden layers: 6\n",
      "Size of last hidden layer: 10\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "LB_NN, UB_NN = perform_linear_over_box_approximation(nn, LB_N0, UB_N0, LB_hidden_box_list, UB_hidden_box_list, verbose = False)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- For Linear (over box) ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[ 0.        ,  0.        ],\n",
      "       [10.0469925 , 10.33118855],\n",
      "       [ 2.42420647,  2.4952212 ],\n",
      "       [ 0.80795125,  0.84694697],\n",
      "       [ 0.06546422,  0.13054258],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 2.9903704 ,  3.13904587],\n",
      "       [ 2.84708954,  2.92978192],\n",
      "       [ 0.        ,  0.        ]])\n",
      "[VERIFY] Verification status: \n",
      "verified\n",
      "----------------------------\n",
      "Calculation time: 0.475586 s.\n"
     ]
    }
   ],
   "source": [
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear (over box) ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN, UB_NN], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)\n",
    "print(\"----------------------------\")\n",
    "print(\"Calculation time: %4f s.\" % (t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Optimize through linear only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "lp_freq = 1\n",
    "lp_start = 1\n",
    "verbose = True\n",
    "\n",
    "lp_list = [i for i in range(lp_start, numlayer, lp_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relu layers: 6\n",
      "Number of hidden layers: 6\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 0.091056\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-1.76, -1.67,   nan,   nan,  0.  ],\n",
      "       [-1.98, -1.87,   nan,   nan,  0.  ],\n",
      "       [-1.46, -1.39,   nan,   nan,  0.  ],\n",
      "       [-0.07,  0.02,   nan,   nan,  0.  ],\n",
      "       [-1.15, -1.05,   nan,   nan,  0.  ],\n",
      "       [ 1.13,  1.22,   nan,   nan,  0.  ],\n",
      "       [-0.73, -0.65,   nan,   nan,  0.  ],\n",
      "       [ 0.51,  0.61,   nan,   nan,  0.  ],\n",
      "       [-1.52, -1.43,   nan,   nan,  0.  ],\n",
      "       [ 1.05,  1.14,   nan,   nan,  0.  ],\n",
      "       [-3.48, -3.39,   nan,   nan,  0.  ],\n",
      "       [-4.13, -4.06,   nan,   nan,  0.  ],\n",
      "       [ 0.97,  1.06,   nan,   nan,  0.  ],\n",
      "       [-0.24, -0.17,   nan,   nan,  0.  ],\n",
      "       [ 1.11,  1.19,   nan,   nan,  0.  ],\n",
      "       [ 0.24,  0.32,   nan,   nan,  0.  ],\n",
      "       [-3.12, -3.03,   nan,   nan,  0.  ],\n",
      "       [ 4.66,  4.75,   nan,   nan,  0.  ],\n",
      "       [-2.72, -2.65,   nan,   nan,  0.  ],\n",
      "       [-0.79, -0.7 ,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riai2018/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:3250: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 0.436735\n",
      "Time per LP neuron 0.021837\n",
      "LP used on 20 neurons.\n",
      "Median global rank: 0.543750\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.6 , -0.57,  0.36,  0.36,  1.  ],\n",
      "       [-0.02,  0.04,  0.91,  0.75,  1.  ],\n",
      "       [-0.49, -0.44,  0.54,  0.16,  1.  ],\n",
      "       [-1.02, -0.96,  0.92,  0.21,  1.  ],\n",
      "       [-0.83, -0.77,  0.53,  0.91,  1.  ],\n",
      "       [ 5.69,  5.83,  0.88,  0.25,  1.  ],\n",
      "       [ 1.65,  1.73,  0.77,  0.71,  1.  ],\n",
      "       [-0.5 , -0.42,  0.52,  0.66,  1.  ],\n",
      "       [-0.6 , -0.56,  0.58,  0.06,  1.  ],\n",
      "       [-0.13, -0.08,  0.48,  0.5 ,  1.  ],\n",
      "       [-0.03,  0.07,  0.22,  0.31,  1.  ],\n",
      "       [-0.24, -0.2 ,  0.19,  0.  ,  1.  ],\n",
      "       [-0.66, -0.6 ,  0.87,  0.61,  1.  ],\n",
      "       [ 1.13,  1.22,  0.62,  0.46,  1.  ],\n",
      "       [ 4.91,  5.03,  0.11,  0.41,  1.  ],\n",
      "       [-0.16, -0.08,  0.28,  0.96,  1.  ],\n",
      "       [-1.14, -1.09,  0.89,  0.11,  1.  ],\n",
      "       [ 0.12,  0.14,  0.56,  0.56,  1.  ],\n",
      "       [-0.74, -0.64,  0.81,  0.81,  1.  ],\n",
      "       [ 1.48,  1.52,  0.08,  0.86,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 0.294255\n",
      "Time per LP neuron 0.014713\n",
      "LP used on 20 neurons.\n",
      "Median global rank: 0.606250\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.04,  0.06,  0.75,  0.91,  1.  ],\n",
      "       [ 0.03,  0.08,  0.25,  0.96,  1.  ],\n",
      "       [ 3.1 ,  3.19,  0.71,  0.11,  1.  ],\n",
      "       [ 0.77,  0.81,  0.06,  0.75,  1.  ],\n",
      "       [-0.43, -0.4 ,  0.5 ,  0.5 ,  1.  ],\n",
      "       [-0.31, -0.29,  0.12,  0.41,  1.  ],\n",
      "       [ 0.3 ,  0.33,  0.31,  0.16,  1.  ],\n",
      "       [ 2.46,  2.51,  0.42,  0.06,  1.  ],\n",
      "       [ 7.64,  7.83,  0.07,  0.56,  1.  ],\n",
      "       [-0.44, -0.4 ,  0.79,  0.21,  1.  ],\n",
      "       [-1.1 , -1.06,  0.47,  0.46,  1.  ],\n",
      "       [-2.08, -2.03,  0.24,  0.86,  1.  ],\n",
      "       [ 1.95,  2.02,  0.96,  0.36,  1.  ],\n",
      "       [ 1.25,  1.29,  0.72,  0.61,  1.  ],\n",
      "       [-0.5 , -0.47,  0.78,  0.71,  1.  ],\n",
      "       [ 6.15,  6.31,  0.63,  0.31,  1.  ],\n",
      "       [ 0.19,  0.22,  0.64,  0.25,  1.  ],\n",
      "       [-2.09, -2.04,  0.94,  0.81,  1.  ],\n",
      "       [ 2.56,  2.62,  0.67,  0.66,  1.  ],\n",
      "       [ 0.05,  0.09,  0.59,  0.  ,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 0.357502\n",
      "Time per LP neuron 0.017875\n",
      "LP used on 20 neurons.\n",
      "Median global rank: 0.356250\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-6.550e+00, -6.380e+00,  1.600e-01,  0.000e+00,  1.000e+00],\n",
      "       [-2.500e-01, -2.400e-01,  4.400e-01,  5.000e-01,  1.000e+00],\n",
      "       [-3.570e+00, -3.480e+00,  3.900e-01,  2.100e-01,  1.000e+00],\n",
      "       [-9.000e-02, -6.000e-02,  3.000e-02,  8.600e-01,  1.000e+00],\n",
      "       [ 4.810e+00,  4.910e+00,  2.300e-01,  3.600e-01,  1.000e+00],\n",
      "       [-1.850e+00, -1.780e+00,  4.900e-01,  7.100e-01,  1.000e+00],\n",
      "       [-3.230e+00, -3.130e+00,  0.000e+00,  3.100e-01,  1.000e+00],\n",
      "       [-6.000e-02, -1.000e-02,  3.300e-01,  5.600e-01,  1.000e+00],\n",
      "       [-2.880e+00, -2.800e+00,  9.300e-01,  9.100e-01,  1.000e+00],\n",
      "       [-8.300e-01, -7.600e-01,  5.700e-01,  4.600e-01,  1.000e+00],\n",
      "       [-9.300e-01, -8.800e-01,  1.400e-01,  8.100e-01,  1.000e+00],\n",
      "       [-3.040e+00, -2.940e+00,  8.300e-01,  7.500e-01,  1.000e+00],\n",
      "       [-5.500e-01, -5.300e-01,  8.400e-01,  9.600e-01,  1.000e+00],\n",
      "       [-1.560e+00, -1.520e+00,  7.400e-01,  2.500e-01,  1.000e+00],\n",
      "       [-3.340e+00, -3.250e+00,  9.800e-01,  6.100e-01,  1.000e+00],\n",
      "       [ 3.660e+00,  3.760e+00,  2.700e-01,  6.600e-01,  1.000e+00],\n",
      "       [ 9.880e+00,  1.012e+01,  6.800e-01,  1.100e-01,  1.000e+00],\n",
      "       [-1.540e+00, -1.460e+00,  1.300e-01,  4.100e-01,  1.000e+00],\n",
      "       [ 5.030e+00,  5.140e+00,  1.800e-01,  1.600e-01,  1.000e+00],\n",
      "       [-5.300e-01, -5.100e-01,  2.000e-02,  6.000e-02,  1.000e+00]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 4\n",
      "Time 0.301596\n",
      "Time per LP neuron 0.015080\n",
      "LP used on 20 neurons.\n",
      "Median global rank: 0.412500\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 2.66,  2.7 ,  0.21,  0.41,  1.  ],\n",
      "       [-1.44, -1.4 ,  0.29,  0.25,  1.  ],\n",
      "       [ 0.1 ,  0.12,  0.66,  0.5 ,  1.  ],\n",
      "       [-4.82, -4.71,  0.04,  0.36,  1.  ],\n",
      "       [-0.78, -0.76,  0.97,  0.  ,  1.  ],\n",
      "       [-4.89, -4.77,  0.38,  0.46,  1.  ],\n",
      "       [-1.06, -1.04,  0.73,  0.75,  1.  ],\n",
      "       [-3.04, -2.94,  0.17,  0.66,  1.  ],\n",
      "       [-7.82, -7.63,  0.61,  0.11,  1.  ],\n",
      "       [-5.6 , -5.46,  0.69,  0.16,  1.  ],\n",
      "       [-4.51, -4.39,  0.46,  0.21,  1.  ],\n",
      "       [-1.28, -1.25,  0.41,  0.81,  1.  ],\n",
      "       [-2.33, -2.28,  0.37,  0.71,  1.  ],\n",
      "       [-2.5 , -2.43,  0.99,  0.96,  1.  ],\n",
      "       [-3.05, -2.98,  0.32,  0.31,  1.  ],\n",
      "       [ 3.9 ,  3.98,  0.34,  0.86,  1.  ],\n",
      "       [ 0.13,  0.16,  0.82,  0.61,  1.  ],\n",
      "       [-2.41, -2.35,  0.43,  0.91,  1.  ],\n",
      "       [ 8.82,  9.04,  0.09,  0.06,  1.  ],\n",
      "       [-2.56, -2.5 ,  0.86,  0.56,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 5\n",
      "Time 0.105443\n",
      "Time per LP neuron 0.010544\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.  , -4.29,   nan,   nan,  1.  ],\n",
      "       [10.1 ,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  2.49,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.84,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.13,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -2.27,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -4.71,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  3.09,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  2.93,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -1.73,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): 7.004310 \n",
      "\n",
      "Margin per LP neuron (more positve is better): 0.077826\n",
      "\n",
      "Margin per second (more positve is better):   4.414703\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "{'time': [0.09105610847473145, 0.4367349147796631, 0.29425549507141113, 0.3575015068054199, 0.30159568786621094, 0.10544300079345703], 'LB_hat': [array([-1.76385256, -1.98753952, -1.46817394, -0.07222603, -1.15187967,\n",
      "        1.12249094, -0.73749013,  0.50628401, -1.520204  ,  1.04931947,\n",
      "       -3.48031755, -4.1383164 ,  0.96801833, -0.24664968,  1.10875178,\n",
      "        0.23588701, -3.12179195,  4.65153167, -2.72709009, -0.79159644]), array([-0.6098033 , -0.02367247, -0.49726519, -1.02907456, -0.83356255,\n",
      "        5.68995823,  1.64670788, -0.50694538, -0.60802405, -0.13246112,\n",
      "       -0.03219188, -0.24763317, -0.66651502,  1.12758099,  4.90874315,\n",
      "       -0.1625087 , -1.14600878,  0.1171228 , -0.7487879 ,  1.47055233]), array([ 0.03531071,  0.02624121,  3.09712527,  0.76891126, -0.43249294,\n",
      "       -0.31867412,  0.2905095 ,  2.45313689,  7.63473203, -0.44572752,\n",
      "       -1.10985752, -2.08748843,  1.94738587,  1.24120517, -0.50702645,\n",
      "        6.14265306,  0.18595096, -2.09354504,  2.55008573,  0.04508633]), array([-6.55137729, -0.25938453, -3.57131864, -0.09778138,  4.80647989,\n",
      "       -1.85242546, -3.23834304, -0.06958181, -2.88592917, -0.83594417,\n",
      "       -0.93098564, -3.04170352, -0.55271958, -1.56680658, -3.34702281,\n",
      "        3.65273433,  9.87395126, -1.54709841,  5.02390464, -0.53038305]), array([ 2.65045075, -1.44682808,  0.09470069, -4.82338441, -0.78222655,\n",
      "       -4.89843515, -1.06883991, -3.04213539, -7.82989658, -5.60106447,\n",
      "       -4.51762506, -1.28179023, -2.33938845, -2.50469917, -3.05632667,\n",
      "        3.89812884,  0.12964305, -2.41468568,  8.81450803, -2.56320861]), array([ 0.        , 10.09297108,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])], 'UB_hat': [array([-1.6787971 , -1.87794102, -1.39226741,  0.01510927, -1.0512356 ,\n",
      "        1.21255568, -0.65883889,  0.60238092, -1.43008016,  1.13703233,\n",
      "       -3.39843506, -4.06364544,  1.05145835, -0.1768291 ,  1.18920342,\n",
      "        0.31314121, -3.03268656,  4.74299019, -2.65916901, -0.70835856]), array([-0.57799584,  0.03134706, -0.4454258 , -0.96739661, -0.77239899,\n",
      "        5.82477699,  1.72815857, -0.42375138, -0.56114727, -0.08151063,\n",
      "        0.06477157, -0.20255024, -0.60736089,  1.21215752,  5.02201342,\n",
      "       -0.0872651 , -1.09863358,  0.13579062, -0.6459027 ,  1.5158776 ]), array([ 0.0505089 ,  0.07510812,  3.18629073,  0.80190108, -0.40512236,\n",
      "       -0.29434716,  0.32430092,  2.5032343 ,  7.82578412, -0.40588257,\n",
      "       -1.06905711, -2.03466526,  2.01205578,  1.28009161, -0.47366095,\n",
      "        6.30526012,  0.21094318, -2.04090043,  2.6126968 ,  0.08079992]), array([-6.38987842, -0.24574129, -3.48047353, -0.06987565,  4.90542726,\n",
      "       -1.78484353, -3.13594319, -0.01585314, -2.80827531, -0.76239221,\n",
      "       -0.88696524, -2.94636286, -0.53303364, -1.52165297, -3.2506057 ,\n",
      "        3.75127067, 10.11518781, -1.46953722,  5.1357791 , -0.51032546]), array([ 2.69283907, -1.40796832,  0.11210949, -4.7132369 , -0.76920152,\n",
      "       -4.77387223, -1.04116811, -2.94415953, -7.63704299, -5.46033367,\n",
      "       -4.39428093, -1.25219022, -2.28680949, -2.43662526, -2.98604903,\n",
      "        3.97363613,  0.15351926, -2.3540368 ,  9.03005919, -2.5041566 ]), array([0.        , 0.        , 2.48018364, 0.8327827 , 0.12350749,\n",
      "       0.        , 0.        , 3.08866122, 2.92154509, 0.        ])], 'use_LP': [array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True]), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True]), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True]), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True,  True]), array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True])], 'local_rank': [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan]), array([0.35, 0.75, 0.15, 0.2 , 0.9 , 0.25, 0.7 , 0.65, 0.05, 0.5 , 0.3 ,\n",
      "       0.  , 0.6 , 0.45, 0.4 , 0.95, 0.1 , 0.55, 0.8 , 0.85]), array([0.9 , 0.95, 0.1 , 0.75, 0.5 , 0.4 , 0.15, 0.05, 0.55, 0.2 , 0.45,\n",
      "       0.85, 0.35, 0.6 , 0.7 , 0.3 , 0.25, 0.8 , 0.65, 0.  ]), array([0.  , 0.5 , 0.2 , 0.85, 0.35, 0.7 , 0.3 , 0.55, 0.9 , 0.45, 0.8 ,\n",
      "       0.75, 0.95, 0.25, 0.6 , 0.65, 0.1 , 0.4 , 0.15, 0.05]), array([0.4 , 0.25, 0.5 , 0.35, 0.  , 0.45, 0.75, 0.65, 0.1 , 0.15, 0.2 ,\n",
      "       0.8 , 0.7 , 0.95, 0.3 , 0.85, 0.6 , 0.9 , 0.05, 0.55]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])], 'global_rank': [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan]), array([0.35  , 0.9   , 0.5375, 0.9125, 0.525 , 0.875 , 0.7625, 0.5125,\n",
      "       0.575 , 0.475 , 0.2125, 0.1875, 0.8625, 0.6125, 0.1   , 0.275 ,\n",
      "       0.8875, 0.55  , 0.8   , 0.075 ]), array([0.75  , 0.25  , 0.7   , 0.05  , 0.5   , 0.1125, 0.3   , 0.4125,\n",
      "       0.0625, 0.7875, 0.4625, 0.2375, 0.95  , 0.7125, 0.775 , 0.625 ,\n",
      "       0.6375, 0.9375, 0.6625, 0.5875]), array([0.15  , 0.4375, 0.3875, 0.025 , 0.225 , 0.4875, 0.    , 0.325 ,\n",
      "       0.925 , 0.5625, 0.1375, 0.825 , 0.8375, 0.7375, 0.975 , 0.2625,\n",
      "       0.675 , 0.125 , 0.175 , 0.0125]), array([0.2   , 0.2875, 0.65  , 0.0375, 0.9625, 0.375 , 0.725 , 0.1625,\n",
      "       0.6   , 0.6875, 0.45  , 0.4   , 0.3625, 0.9875, 0.3125, 0.3375,\n",
      "       0.8125, 0.425 , 0.0875, 0.85  ]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])], 'margin': [7.0043098589346755], 'margin_per_neuron': [0.07782566509927417], 'margin_per_time': [4.41470346250348]}\n"
     ]
    }
   ],
   "source": [
    "t_1 =time.time()\n",
    "LB_NN, UB_NN, stats = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list,\n",
    "                             LB_hidden_box_list, UB_hidden_box_list, label, verbose=verbose)\n",
    "t_2 =time.time()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------TIME------------\n",
      "Total number of layers = 6\n",
      "lp_list = [1, 2, 3, 4, 5]\n",
      "Time Solving LP: 1.660013 s.\n",
      "----------------------------\n",
      "--------- For Linear ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[ 0.        ,  0.        ],\n",
      "       [10.09297108,  0.        ],\n",
      "       [ 0.        ,  2.48018364],\n",
      "       [ 0.        ,  0.8327827 ],\n",
      "       [ 0.        ,  0.12350749],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  3.08866122],\n",
      "       [ 0.        ,  2.92154509],\n",
      "       [ 0.        ,  0.        ]])\n",
      "[VERIFY] Verification status: \n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "print(\"------------TIME------------\")\n",
    "print(\"Total number of layers = %s\"% numlayer)\n",
    "print(\"lp_list = %s\"% lp_list)\n",
    "print(\"Time Solving LP: %4f s.\" % (t_2-t_1))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN.squeeze(), UB_NN.squeeze()], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Speed Linear Solver up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "lp_freq = 2\n",
    "lp_start = 1\n",
    "verbose = True\n",
    "\n",
    "lp_list = [i for i in range(lp_start, numlayer, lp_freq)]\n",
    "lp_list = [i for i in range(lp_start, numlayer-2)]\n",
    "print(lp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-83b4c32f7a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m LB_NN, UB_NN, stats = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list,\n\u001b[1;32m      3\u001b[0m                              \u001b[0mLB_hidden_box_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUB_hidden_box_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_rank_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                        local_rank_threshold=0.0)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-134b86c45bf8>\u001b[0m in \u001b[0;36mperform_linear_layerwise\u001b[0;34m(nn, numlayer, LB_N0, UB_N0, lp_list, LB_hidden_box_list, UB_hidden_box_list, true_label, global_rank_threshold, local_rank_threshold, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mLB_hidden_box_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mUB_hidden_box_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_rank_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_rank_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m<=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobal_rank_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "t_1 =time.time()\n",
    "LB_NN, UB_NN, stats = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list,\n",
    "                             LB_hidden_box_list, UB_hidden_box_list, label, verbose=verbose, global_rank_threshold=0.0,\n",
    "                                       local_rank_threshold=0.0)\n",
    "t_2 =time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------TIME------------\")\n",
    "print(\"Total number of layers = %s\"% numlayer)\n",
    "print(\"lp_list = %s\"% lp_list)\n",
    "print(\"Time Solving LP: %4f s.\" % (t_2-t_1))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN.squeeze(), UB_NN.squeeze()], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_nn_and_write(net_code, img_nrs, epsilon, log_file=None, verbose=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Analyse Neural Network and write the result to a log file. Code heuristic yourself in this function\n",
    "    INPUT:\n",
    "        - net_code: Neural Network code, e.g. mnist_relu_4_1024\n",
    "        - img_nrs: List of image nrs, e.g [0,1,2,3]\n",
    "        - epsilon: Perturbation\n",
    "        - log_file: File to write results\n",
    "    OUTPUT:\n",
    "        - verified_flag: Verification flag. True if network was verified against perturbation.\n",
    "    \"\"\"\n",
    "    nn_root_path = '/home/riai2018/mnist_nets'\n",
    "    img_root_path = '/home/riai2018/mnist_images'\n",
    "    log_root_path = '/home/riai2018/log'\n",
    "    log_base_name = 'log_'\n",
    "    verified_counter, true_label_counter = 0, 0\n",
    "    stamp = '_{:%Y-%m-%d_%H:%M:%S}'.format(datetime.now())\n",
    "    if not os.path.isdir(log_root_path):\n",
    "        os.makedirs(log_root_path)\n",
    "    net_path = os.path.join(nn_root_path, net_code + '.txt')\n",
    "    if log_file is None:\n",
    "        log_file = os.path.join(log_root_path, log_base_name + net_code + '_epsilon_' + str(epsilon))\n",
    "        if kwargs is not None:\n",
    "            log_file = log_file + '_'  + str(kwargs) + \"_\"\n",
    "        log_file = log_file + stamp +'.txt'\n",
    "            \n",
    "    with open(log_file, \"+a\") as f:\n",
    "        f.write(\"Epsilon %s kwargs: %s \\n\\n\" % (epsilon, kwargs))\n",
    "        \n",
    "    assert max(img_nrs) <= 99\n",
    "    assert min(img_nrs) >= 0\n",
    "    t_start = time.time()\n",
    "    \n",
    "    runtimes = []\n",
    "    \n",
    "    for img_nr in img_nrs:\n",
    "        img_path = os.path.join(img_root_path, 'img' + str(img_nr) + '.txt')\n",
    "\n",
    "        # Load NN and perturbe image\n",
    "        LB_N0, UB_N0, nn, label, flag_wrong_label = load_nn(net_path, img_path, epsilon)\n",
    "        true_label_counter += int(not flag_wrong_label)\n",
    "        numlayer = nn.numlayer\n",
    "\n",
    "        # You heuristics come here:\n",
    "#         lp_freq = 2\n",
    "        lp_start = 1\n",
    "        numlayer = nn.numlayer\n",
    "#         lp_list = [i for i in range(lp_start, numlayer, lp_freq)]\n",
    "        lp_list = [i for i in range(lp_start, numlayer-2)]\n",
    "\n",
    "        # Get Bounds\n",
    "        t1 =time.time()\n",
    "        LB_hidden_box_list, UB_hidden_box_list, LB_NN, UB_NN = perform_box_analysis(nn, LB_N0, UB_N0, verbose = False)\n",
    "        LB_NN, UB_NN, stats = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list,\n",
    "                                     LB_hidden_box_list, UB_hidden_box_list, label, verbose=verbose, **kwargs)\n",
    "        # Check if NN was verified\n",
    "        _, verified_flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, label, num_input_pixels = len(LB_N0), num_out_pixels = 10)\n",
    "        verified_counter += int(verified_flag)\n",
    "       \n",
    "        t2 = time.time()\n",
    "        \n",
    "        runtimes.append(t2-t1)\n",
    "        \n",
    "        # Write to logfile\n",
    "        with open(log_file, \"+a\") as f:\n",
    "            if not flag_wrong_label and verified_flag:\n",
    "                line = \"{:>10} img_{:>2} verified label {:>2} time {:>5.4f} s\\n\".format(net_code, img_nr, label, t2-t1)\n",
    "            elif not flag_wrong_label and not verified_flag:\n",
    "                line = \"{:>10} img_{:>2} failed label {:>2}\\n\".format(net_code, img_nr, label)\n",
    "            else:\n",
    "                line = \"{:>10} img_{:>2} not considered\\n\".format(net_code, img_nr)\n",
    "            f.write(line)\n",
    "            f.write(\"layerwise_time: %s \\n\" % stats['time'])\n",
    "            bounds = np.stack([stats['LB_hat'][-1], stats['UB_hat'][-1]], axis = 1) \n",
    "            f.write(\"Final Bounds: %s \\n\" % (bounds))\n",
    "            f.write(\"Final Verification Margin (negative is not verified): %3f \\n\" % stats['margin'][-1])\n",
    "            f.write(\"Margin per Neuron (more positive is better): %9f \\n\" % stats['margin_per_neuron'][-1])\n",
    "            f.write(\"Margin per second (more positve is better): %10f\\n\" % stats['margin_per_time'][-1])\n",
    "            f.write(\"Median tigthness of hat bounds per layer: %s \\n\" % stats['median_tightness_hat'])\n",
    "            f.write(\"Min tigthness of hat bounds per layer: %s \\n\" % stats['max_tightness_hat'])\n",
    "            f.write(\"Max tigthness of hat bounds per layer: %s \\n\" % stats['min_tightness_hat'])\n",
    "\n",
    "            f.write(\"\\n------------------------------------------------------\\n\")\n",
    "\n",
    "            \n",
    "    with open(log_file, \"+a\") as f:\n",
    "        line = []\n",
    "        line.append(\"\\n--------------------- \\n\")\n",
    "        line.append(\"analysis precision  {:>2} /  {:>2}\\n\".format(verified_counter, true_label_counter))\n",
    "        line.append(\"Average Time: {:>5.4f} \\n\".format((time.time() - t_start)/len(img_nrs)))\n",
    "        line.append(\"Max Time: {:>5.4f} \\n\".format(max(runtimes)))\n",
    "        line.append(\"Min Time: {:>5.4f} \\n\".format(min(runtimes)))\n",
    "        for l in line:\n",
    "            f.write(l)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Neural Networks: ['mnist_relu_3_10', 'mnist_relu_3_20', 'mnist_relu_3_50', 'mnist_relu_4_1024', 'mnist_relu_6_100', 'mnist_relu_6_20', 'mnist_relu_6_200', 'mnist_relu_6_50', 'mnist_relu_9_100', 'mnist_relu_9_200']\n"
     ]
    }
   ],
   "source": [
    "# Create list of all NN and images\n",
    "\n",
    "directory = '../mnist_nets/'\n",
    "nn_list  = os.listdir(directory)\n",
    "nn_list = [f.replace('.txt','') for f in nn_list]\n",
    "nn_list.sort()\n",
    "print(\"All Neural Networks: %s\" % nn_list)\n",
    "directory = '../mnist_images/'\n",
    "img_nrs  = np.arange(100).tolist()\n",
    "verbose = False\n",
    "# Create list of all groundtruth NN and epsilons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([4,2,7,1])\n",
    "temp = np.argsort(array)\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(array))\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 1\n",
      "Number of relu layers: 6\n",
      "Number of hidden layers: 6\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 0.032425\n",
      "Median global rank: nan\n",
      "Median tigthness of hat bounds: 0.084248 \n",
      "\n",
      "Min tigthness of hat bounds: 0.067921 \n",
      "\n",
      "Max tigthness of hat bounds: 0.109598 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-1.76, -1.67,   nan,   nan,  0.  ],\n",
      "       [-1.98, -1.87,   nan,   nan,  0.  ],\n",
      "       [-1.46, -1.39,   nan,   nan,  0.  ],\n",
      "       [-0.07,  0.02,   nan,   nan,  0.  ],\n",
      "       [-1.15, -1.05,   nan,   nan,  0.  ],\n",
      "       [ 1.13,  1.22,   nan,   nan,  0.  ],\n",
      "       [-0.73, -0.65,   nan,   nan,  0.  ],\n",
      "       [ 0.51,  0.61,   nan,   nan,  0.  ],\n",
      "       [-1.52, -1.43,   nan,   nan,  0.  ],\n",
      "       [ 1.05,  1.14,   nan,   nan,  0.  ],\n",
      "       [-3.48, -3.39,   nan,   nan,  0.  ],\n",
      "       [-4.13, -4.06,   nan,   nan,  0.  ],\n",
      "       [ 0.97,  1.06,   nan,   nan,  0.  ],\n",
      "       [-0.24, -0.17,   nan,   nan,  0.  ],\n",
      "       [ 1.11,  1.19,   nan,   nan,  0.  ],\n",
      "       [ 0.24,  0.32,   nan,   nan,  0.  ],\n",
      "       [-3.12, -3.03,   nan,   nan,  0.  ],\n",
      "       [ 4.66,  4.75,   nan,   nan,  0.  ],\n",
      "       [-2.72, -2.65,   nan,   nan,  0.  ],\n",
      "       [-0.79, -0.7 ,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 0.150482\n",
      "Time per LP neuron 0.011576\n",
      "LP used on 13 neurons.\n",
      "Median global rank: 0.181250\n",
      "Median tigthness of hat bounds: 0.061421 \n",
      "\n",
      "Min tigthness of hat bounds: 0.031807 \n",
      "\n",
      "Max tigthness of hat bounds: 0.257538 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.6 , -0.57,  0.16,  0.  ,  1.  ],\n",
      "       [-0.02,  0.04,  0.02,  0.  ,  1.  ],\n",
      "       [-0.52, -0.41,  0.34,  0.  ,  0.  ],\n",
      "       [-1.02, -0.96,  0.03,  0.  ,  1.  ],\n",
      "       [-0.83, -0.77,  0.17,  0.  ,  1.  ],\n",
      "       [ 5.63,  5.89,  0.33,  0.  ,  0.  ],\n",
      "       [ 1.62,  1.77,  0.59,  0.  ,  0.  ],\n",
      "       [-0.5 , -0.42,  0.07,  0.  ,  1.  ],\n",
      "       [-0.6 , -0.56,  0.06,  0.  ,  1.  ],\n",
      "       [-0.13, -0.08,  0.09,  0.  ,  1.  ],\n",
      "       [-0.05,  0.09,  0.61,  0.  ,  0.  ],\n",
      "       [-0.24, -0.2 ,  0.19,  0.  ,  1.  ],\n",
      "       [-0.66, -0.6 ,  0.18,  0.  ,  1.  ],\n",
      "       [ 1.1 ,  1.25,  0.49,  0.  ,  0.  ],\n",
      "       [ 4.91,  5.03,  0.24,  0.  ,  1.  ],\n",
      "       [-0.16, -0.08,  0.25,  0.  ,  1.  ],\n",
      "       [-1.14, -1.09,  0.11,  0.  ,  1.  ],\n",
      "       [ 0.11,  0.15,  0.44,  0.  ,  0.  ],\n",
      "       [-0.78, -0.61,  0.82,  0.  ,  0.  ],\n",
      "       [ 1.48,  1.52,  0.14,  0.  ,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 0.094583\n",
      "Time per LP neuron 0.023646\n",
      "LP used on 4 neurons.\n",
      "Median global rank: 0.600000\n",
      "Median tigthness of hat bounds: 0.100753 \n",
      "\n",
      "Min tigthness of hat bounds: 0.051226 \n",
      "\n",
      "Max tigthness of hat bounds: 0.193472 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.02,  0.07,  0.31,  0.  ,  0.  ],\n",
      "       [ 0.01,  0.11,  0.41,  0.  ,  0.  ],\n",
      "       [ 3.06,  3.24,  0.87,  0.  ,  0.  ],\n",
      "       [ 0.74,  0.85,  0.39,  0.  ,  0.  ],\n",
      "       [-0.46, -0.37,  0.37,  0.  ,  0.  ],\n",
      "       [-0.34, -0.26,  0.68,  0.  ,  0.  ],\n",
      "       [ 0.28,  0.35,  0.81,  0.  ,  0.  ],\n",
      "       [ 2.46,  2.51,  0.22,  0.  ,  1.  ],\n",
      "       [ 7.64,  7.83,  0.21,  0.  ,  1.  ],\n",
      "       [-0.48, -0.37,  0.89,  0.  ,  0.  ],\n",
      "       [-1.16, -1.01,  0.53,  0.  ,  0.  ],\n",
      "       [-2.11, -2.  ,  0.86,  0.  ,  0.  ],\n",
      "       [ 1.92,  2.05,  0.74,  0.  ,  0.  ],\n",
      "       [ 1.22,  1.31,  0.43,  0.  ,  0.  ],\n",
      "       [-0.53, -0.45,  0.69,  0.  ,  0.  ],\n",
      "       [ 6.15,  6.31,  0.08,  0.  ,  1.  ],\n",
      "       [ 0.15,  0.25,  0.97,  0.  ,  0.  ],\n",
      "       [-2.12, -2.01,  0.99,  0.  ,  0.  ],\n",
      "       [ 2.56,  2.62,  0.  ,  0.  ,  1.  ],\n",
      "       [ 0.02,  0.12,  0.78,  0.  ,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 0.060380\n",
      "Time per LP neuron 0.020127\n",
      "LP used on 3 neurons.\n",
      "Median global rank: 0.587500\n",
      "Median tigthness of hat bounds: 0.242534 \n",
      "\n",
      "Min tigthness of hat bounds: 0.020866 \n",
      "\n",
      "Max tigthness of hat bounds: 0.354060 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-6.63, -6.31,  0.98,  0.  ,  0.  ],\n",
      "       [-0.32, -0.18,  0.67,  0.  ,  0.  ],\n",
      "       [-3.57, -3.48,  0.29,  0.  ,  1.  ],\n",
      "       [-0.14, -0.02,  0.88,  0.  ,  0.  ],\n",
      "       [ 4.75,  4.98,  0.48,  0.  ,  0.  ],\n",
      "       [-1.94, -1.69,  0.83,  0.  ,  0.  ],\n",
      "       [-3.32, -3.07,  0.63,  0.  ,  0.  ],\n",
      "       [-0.16,  0.1 ,  0.79,  0.  ,  0.  ],\n",
      "       [-2.88, -2.8 ,  0.23,  0.  ,  1.  ],\n",
      "       [-0.93, -0.66,  0.46,  0.  ,  0.  ],\n",
      "       [-1.07, -0.73,  0.36,  0.  ,  0.  ],\n",
      "       [-3.11, -2.86,  0.71,  0.  ,  0.  ],\n",
      "       [-0.61, -0.46,  0.62,  0.  ,  0.  ],\n",
      "       [-1.64, -1.45,  0.93,  0.  ,  0.  ],\n",
      "       [-3.41, -3.17,  0.91,  0.  ,  0.  ],\n",
      "       [ 3.6 ,  3.81,  0.57,  0.  ,  0.  ],\n",
      "       [ 9.82, 10.17,  0.54,  0.  ,  0.  ],\n",
      "       [-1.63, -1.36,  0.56,  0.  ,  0.  ],\n",
      "       [ 4.93,  5.24,  0.32,  0.  ,  0.  ],\n",
      "       [-0.53, -0.51,  0.04,  0.  ,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 4\n",
      "Time 0.008222\n",
      "Median global rank: 0.606250\n",
      "Median tigthness of hat bounds: 0.211387 \n",
      "\n",
      "Min tigthness of hat bounds: 0.077695 \n",
      "\n",
      "Max tigthness of hat bounds: 0.417041 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 2.54,  2.79,  0.28,  0.  ,  0.  ],\n",
      "       [-1.47, -1.38,  0.94,  0.  ,  0.  ],\n",
      "       [ 0.01,  0.21,  0.92,  0.  ,  0.  ],\n",
      "       [-4.9 , -4.65,  0.84,  0.  ,  0.  ],\n",
      "       [-0.87, -0.64,  0.42,  0.  ,  0.  ],\n",
      "       [-4.99, -4.69,  0.38,  0.  ,  0.  ],\n",
      "       [-1.13, -0.93,  0.72,  0.  ,  0.  ],\n",
      "       [-3.09, -2.89,  0.12,  0.  ,  0.  ],\n",
      "       [-7.95, -7.55,  0.96,  0.  ,  0.  ],\n",
      "       [-5.7 , -5.37,  0.75,  0.  ,  0.  ],\n",
      "       [-4.59, -4.35,  0.27,  0.  ,  0.  ],\n",
      "       [-1.31, -1.23,  0.52,  0.  ,  0.  ],\n",
      "       [-2.38, -2.24,  0.73,  0.  ,  0.  ],\n",
      "       [-2.54, -2.35,  0.77,  0.  ,  0.  ],\n",
      "       [-3.11, -2.94,  0.64,  0.  ,  0.  ],\n",
      "       [ 3.78,  4.1 ,  0.5 ,  0.  ,  0.  ],\n",
      "       [ 0.03,  0.25,  0.13,  0.  ,  0.  ],\n",
      "       [-2.44, -2.3 ,  0.58,  0.  ,  0.  ],\n",
      "       [ 8.71,  9.12,  0.66,  0.  ,  0.  ],\n",
      "       [-2.62, -2.45,  0.47,  0.  ,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 5\n",
      "Time 0.133617\n",
      "Time per LP neuron 0.013362\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "Median tigthness of hat bounds: -0.805406 \n",
      "\n",
      "Min tigthness of hat bounds: -10.078905 \n",
      "\n",
      "Max tigthness of hat bounds: 3.088661 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.  , -4.29,   nan,   nan,  1.  ],\n",
      "       [10.08,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  2.49,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.84,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.13,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -2.26,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -4.7 ,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  3.09,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  2.93,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -1.73,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): 6.990244 \n",
      "\n",
      "Margin per LP neuron (more positve is better): 0.233008\n",
      "\n",
      "Margin per second (more positve is better):  14.571852\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "verified\n",
      "True label: 0\n",
      "Number of relu layers: 6\n",
      "Number of hidden layers: 6\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 0.059845\n",
      "Median global rank: nan\n",
      "Median tigthness of hat bounds: 0.094487 \n",
      "\n",
      "Min tigthness of hat bounds: 0.075145 \n",
      "\n",
      "Max tigthness of hat bounds: 0.117509 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ -1.26,  -1.17,    nan,    nan,   0.  ],\n",
      "       [  7.8 ,   7.92,    nan,    nan,   0.  ],\n",
      "       [  6.54,   6.62,    nan,    nan,   0.  ],\n",
      "       [ -4.7 ,  -4.6 ,    nan,    nan,   0.  ],\n",
      "       [ -1.84,  -1.73,    nan,    nan,   0.  ],\n",
      "       [ -6.58,  -6.47,    nan,    nan,   0.  ],\n",
      "       [ -7.99,  -7.9 ,    nan,    nan,   0.  ],\n",
      "       [ -2.71,  -2.6 ,    nan,    nan,   0.  ],\n",
      "       [  1.41,   1.51,    nan,    nan,   0.  ],\n",
      "       [  8.43,   8.53,    nan,    nan,   0.  ],\n",
      "       [ -3.16,  -3.07,    nan,    nan,   0.  ],\n",
      "       [  4.95,   5.04,    nan,    nan,   0.  ],\n",
      "       [ -3.13,  -3.04,    nan,    nan,   0.  ],\n",
      "       [ -5.51,  -5.43,    nan,    nan,   0.  ],\n",
      "       [ -3.83,  -3.74,    nan,    nan,   0.  ],\n",
      "       [ -0.88,  -0.79,    nan,    nan,   0.  ],\n",
      "       [  2.08,   2.17,    nan,    nan,   0.  ],\n",
      "       [-12.64, -12.53,    nan,    nan,   0.  ],\n",
      "       [ -0.36,  -0.28,    nan,    nan,   0.  ],\n",
      "       [  0.23,   0.33,    nan,    nan,   0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 0.141549\n",
      "Time per LP neuron 0.010888\n",
      "LP used on 13 neurons.\n",
      "Median global rank: 0.181250\n",
      "Median tigthness of hat bounds: 0.086756 \n",
      "\n",
      "Min tigthness of hat bounds: 0.025331 \n",
      "\n",
      "Max tigthness of hat bounds: 0.202458 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-1.42, -1.34,  0.16,  0.  ,  1.  ],\n",
      "       [-5.37, -5.27,  0.02,  0.  ,  1.  ],\n",
      "       [-1.09, -0.99,  0.34,  0.  ,  0.  ],\n",
      "       [ 6.29,  6.4 ,  0.03,  0.  ,  1.  ],\n",
      "       [ 8.97,  9.12,  0.17,  0.  ,  1.  ],\n",
      "       [-3.9 , -3.81,  0.33,  0.  ,  0.  ],\n",
      "       [-2.87, -2.77,  0.59,  0.  ,  0.  ],\n",
      "       [-1.73, -1.67,  0.07,  0.  ,  1.  ],\n",
      "       [ 4.17,  4.24,  0.06,  0.  ,  1.  ],\n",
      "       [-3.66, -3.59,  0.09,  0.  ,  1.  ],\n",
      "       [-5.28, -5.08,  0.61,  0.  ,  0.  ],\n",
      "       [-1.99, -1.96,  0.19,  0.  ,  1.  ],\n",
      "       [-1.66, -1.59,  0.18,  0.  ,  1.  ],\n",
      "       [-0.4 , -0.21,  0.49,  0.  ,  0.  ],\n",
      "       [-3.99, -3.92,  0.24,  0.  ,  1.  ],\n",
      "       [-3.76, -3.62,  0.25,  0.  ,  1.  ],\n",
      "       [-0.85, -0.82,  0.11,  0.  ,  1.  ],\n",
      "       [-0.05, -0.01,  0.44,  0.  ,  0.  ],\n",
      "       [-5.16, -4.95,  0.82,  0.  ,  0.  ],\n",
      "       [-3.53, -3.5 ,  0.14,  0.  ,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 0.078468\n",
      "Time per LP neuron 0.019617\n",
      "LP used on 4 neurons.\n",
      "Median global rank: 0.600000\n",
      "Median tigthness of hat bounds: 0.053511 \n",
      "\n",
      "Min tigthness of hat bounds: 0.014787 \n",
      "\n",
      "Max tigthness of hat bounds: 0.136351 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.73, -0.72,  0.31,  0.  ,  0.  ],\n",
      "       [-4.52, -4.43,  0.41,  0.  ,  0.  ],\n",
      "       [ 4.36,  4.44,  0.87,  0.  ,  0.  ],\n",
      "       [-1.79, -1.72,  0.39,  0.  ,  0.  ],\n",
      "       [ 0.1 ,  0.13,  0.37,  0.  ,  0.  ],\n",
      "       [-2.42, -2.38,  0.68,  0.  ,  0.  ],\n",
      "       [ 1.26,  1.28,  0.81,  0.  ,  0.  ],\n",
      "       [-2.67, -2.64,  0.22,  0.  ,  1.  ],\n",
      "       [-3.21, -3.16,  0.21,  0.  ,  1.  ],\n",
      "       [ 5.47,  5.57,  0.89,  0.  ,  0.  ],\n",
      "       [-2.42, -2.36,  0.53,  0.  ,  0.  ],\n",
      "       [ 0.18,  0.23,  0.86,  0.  ,  0.  ],\n",
      "       [-3.29, -3.23,  0.74,  0.  ,  0.  ],\n",
      "       [ 0.78,  0.81,  0.43,  0.  ,  0.  ],\n",
      "       [-0.75, -0.69,  0.69,  0.  ,  0.  ],\n",
      "       [-3.22, -3.18,  0.08,  0.  ,  1.  ],\n",
      "       [ 3.81,  3.92,  0.97,  0.  ,  0.  ],\n",
      "       [ 7.61,  7.75,  0.99,  0.  ,  0.  ],\n",
      "       [-1.89, -1.85,  0.  ,  0.  ,  1.  ],\n",
      "       [ 3.6 ,  3.66,  0.78,  0.  ,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 0.060942\n",
      "Time per LP neuron 0.020314\n",
      "LP used on 3 neurons.\n",
      "Median global rank: 0.587500\n",
      "Median tigthness of hat bounds: 0.097404 \n",
      "\n",
      "Min tigthness of hat bounds: 0.011275 \n",
      "\n",
      "Max tigthness of hat bounds: 0.153548 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-3.55, -3.45,  0.98,  0.  ,  0.  ],\n",
      "       [ 0.84,  0.9 ,  0.67,  0.  ,  0.  ],\n",
      "       [-0.79, -0.77,  0.29,  0.  ,  1.  ],\n",
      "       [ 3.9 ,  3.99,  0.88,  0.  ,  0.  ],\n",
      "       [-2.16, -2.03,  0.48,  0.  ,  0.  ],\n",
      "       [ 1.84,  1.93,  0.83,  0.  ,  0.  ],\n",
      "       [-0.49, -0.38,  0.63,  0.  ,  0.  ],\n",
      "       [ 4.23,  4.35,  0.79,  0.  ,  0.  ],\n",
      "       [-2.18, -2.14,  0.23,  0.  ,  1.  ],\n",
      "       [-2.81, -2.74,  0.46,  0.  ,  0.  ],\n",
      "       [ 0.52,  0.66,  0.36,  0.  ,  0.  ],\n",
      "       [-6.01, -5.86,  0.71,  0.  ,  0.  ],\n",
      "       [-2.11, -2.02,  0.62,  0.  ,  0.  ],\n",
      "       [ 1.28,  1.36,  0.93,  0.  ,  0.  ],\n",
      "       [-5.86, -5.72,  0.91,  0.  ,  0.  ],\n",
      "       [ 6.77,  6.92,  0.57,  0.  ,  0.  ],\n",
      "       [-0.88, -0.8 ,  0.54,  0.  ,  0.  ],\n",
      "       [-6.41, -6.26,  0.56,  0.  ,  0.  ],\n",
      "       [-5.7 , -5.59,  0.32,  0.  ,  0.  ],\n",
      "       [-0.1 , -0.09,  0.04,  0.  ,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "[Network] Input pixels: 20\n",
      "[Network] Shape of weights: (20, 20)\n",
      "[Network] Shape of biases: (20,)\n",
      "[Network] Out pixels: 20\n",
      "--------------------------\n",
      "Layerno: 4\n",
      "Time 0.009034\n",
      "Median global rank: 0.606250\n",
      "Median tigthness of hat bounds: 0.126143 \n",
      "\n",
      "Min tigthness of hat bounds: 0.062802 \n",
      "\n",
      "Max tigthness of hat bounds: 0.201119 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-4.52, -4.35,  0.28,  0.  ,  0.  ],\n",
      "       [-1.23, -1.16,  0.94,  0.  ,  0.  ],\n",
      "       [ 0.4 ,  0.54,  0.92,  0.  ,  0.  ],\n",
      "       [-2.57, -2.44,  0.84,  0.  ,  0.  ],\n",
      "       [ 6.14,  6.31,  0.42,  0.  ,  0.  ],\n",
      "       [-1.86, -1.7 ,  0.38,  0.  ,  0.  ],\n",
      "       [-1.79, -1.61,  0.72,  0.  ,  0.  ],\n",
      "       [-3.59, -3.41,  0.12,  0.  ,  0.  ],\n",
      "       [-2.17, -1.97,  0.96,  0.  ,  0.  ],\n",
      "       [ 0.89,  0.98,  0.75,  0.  ,  0.  ],\n",
      "       [-2.15, -2.03,  0.27,  0.  ,  0.  ],\n",
      "       [-1.08, -0.99,  0.52,  0.  ,  0.  ],\n",
      "       [-1.32, -1.21,  0.73,  0.  ,  0.  ],\n",
      "       [-2.81, -2.62,  0.77,  0.  ,  0.  ],\n",
      "       [-2.19, -2.12,  0.64,  0.  ,  0.  ],\n",
      "       [-0.2 , -0.13,  0.5 ,  0.  ,  0.  ],\n",
      "       [ 0.94,  1.1 ,  0.13,  0.  ,  0.  ],\n",
      "       [ 0.53,  0.6 ,  0.58,  0.  ,  0.  ],\n",
      "       [-0.14, -0.05,  0.66,  0.  ,  0.  ],\n",
      "       [-0.26, -0.19,  0.47,  0.  ,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 5\n",
      "Time 0.097628\n",
      "Time per LP neuron 0.009763\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "Median tigthness of hat bounds: 0.097872 \n",
      "\n",
      "Min tigthness of hat bounds: -6.391774 \n",
      "\n",
      "Max tigthness of hat bounds: 1.951687 \n",
      "\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 6.4 ,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -4.36,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.8 ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -0.01,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -3.52,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  1.96,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.62,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  1.08,   nan,   nan,  1.  ],\n",
      "       [ 0.  , -2.84,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  0.22,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): 4.440087 \n",
      "\n",
      "Margin per LP neuron (more positve is better): 0.148003\n",
      "\n",
      "Margin per second (more positve is better):   9.922755\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': [0.059844970703125,\n",
       "  0.14154863357543945,\n",
       "  0.07846832275390625,\n",
       "  0.06094193458557129,\n",
       "  0.009033679962158203,\n",
       "  0.09762763977050781],\n",
       " 'LB_hat': [array([ -1.26549334,   7.79631979,   6.53588223,  -4.70052856,\n",
       "          -1.84236632,  -6.58231054,  -7.99787284,  -2.71496696,\n",
       "           1.40617831,   8.4212984 ,  -3.16152067,   4.94829639,\n",
       "          -3.13571021,  -5.51140898,  -3.83282426,  -0.88180955,\n",
       "           2.07318787, -12.64210322,  -0.36338058,   0.22995437]),\n",
       "  array([-1.42051807, -5.37413963, -1.0961845 ,  6.2877356 ,  8.9673463 ,\n",
       "         -3.9073478 , -2.87461919, -1.73783682,  4.16404499, -3.66497027,\n",
       "         -5.28039444, -1.99541948, -1.66445968, -0.40627105, -3.99842559,\n",
       "         -3.76938187, -0.85056194, -0.05134394, -5.16081344, -3.53933892]),\n",
       "  array([-0.73816649, -4.52844373,  4.35185494, -1.794051  ,  0.0954157 ,\n",
       "         -2.42610735,  1.25077603, -2.67939111, -3.21681654,  5.46782112,\n",
       "         -2.4290538 ,  0.17761417, -3.29416781,  0.77548775, -0.75256349,\n",
       "         -3.22735537,  3.80754363,  7.60452117, -1.89784333,  3.59686624]),\n",
       "  array([-3.55809661,  0.83668708, -0.79251939,  3.89523646, -2.16916905,\n",
       "          1.83456783, -0.49568118,  4.2274489 , -2.18149083, -2.81798932,\n",
       "          0.51398616, -6.01483226, -2.11020821,  1.2725788 , -5.86122319,\n",
       "          6.762423  , -0.8899575 , -6.413744  , -5.70980404, -0.10920974]),\n",
       "  array([-4.52655959, -1.23967832,  0.39205704, -2.57578938,  6.13074611,\n",
       "         -1.86832738, -1.79456586, -3.59468606, -2.17171261,  0.88197334,\n",
       "         -2.15727367, -1.08164742, -1.32928784, -2.81099731, -2.19967962,\n",
       "         -0.20111946,  0.93165064,  0.52601087, -0.14429562, -0.26230408]),\n",
       "  array([6.39177426, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ])],\n",
       " 'UB_hat': [array([ -1.17124703,   7.91382858,   6.61990488,  -4.60124735,\n",
       "          -1.73030913,  -6.47899859,  -7.90603671,  -2.60733965,\n",
       "           1.50335569,   8.52330289,  -3.07220904,   5.03112562,\n",
       "          -3.04253861,  -5.43174253,  -3.74517361,  -0.79585329,\n",
       "           2.16939173, -12.5390785 ,  -0.28823524,   0.32468218]),\n",
       "  array([-1.3419137 , -5.27259542, -0.99419164,  6.39540707,  9.11473466,\n",
       "         -3.81243977, -2.77199373, -1.67124274,  4.23443813, -3.59279321,\n",
       "         -5.08113196, -1.96142058, -1.59575393, -0.21164951, -3.92315802,\n",
       "         -3.62849599, -0.82523122, -0.01028027, -4.95835509, -3.50092519]),\n",
       "  array([-0.7233798 , -4.43472093,  4.4371053 , -1.7291708 ,  0.12041543,\n",
       "         -2.38563396,  1.27573157, -2.64032757, -3.16751915,  5.56245618,\n",
       "         -2.36326503,  0.22683421, -3.2330621 ,  0.80193627, -0.69212404,\n",
       "         -3.18466679,  3.91649927,  7.74087262, -1.8505124 ,  3.65459137]),\n",
       "  array([-3.45476804,  0.89882897, -0.77146674,  3.98546561, -2.03208282,\n",
       "          1.92604667, -0.38971378,  4.34450783, -2.14366817, -2.74069784,\n",
       "          0.65614141, -5.86774249, -2.02785671,  1.35617577, -5.7206478 ,\n",
       "          6.9159711 , -0.80059297, -6.26540066, -5.59046029, -0.09793504]),\n",
       "  array([-4.35306649, -1.16462857,  0.53495325, -2.44622904,  6.30507446,\n",
       "         -1.70482293, -1.61789625, -3.41274335, -1.97059383,  0.97821744,\n",
       "         -2.03454807, -0.99951715, -1.21085161, -2.6239779 , -2.12000051,\n",
       "         -0.13831749,  1.09067397,  0.59828887, -0.05347491, -0.19578942]),\n",
       "  array([0.        , 0.        , 0.79552876, 0.        , 0.        ,\n",
       "         1.95168688, 0.61189154, 1.07898203, 0.        , 0.21486805])],\n",
       " 'use_LP': [array([False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,\n",
       "         False, False]),\n",
       "  array([ True,  True, False,  True,  True, False, False,  True,  True,\n",
       "          True, False,  True,  True, False,  True,  True,  True, False,\n",
       "         False,  True]),\n",
       "  array([False, False, False, False, False, False, False,  True,  True,\n",
       "         False, False, False, False, False, False,  True, False, False,\n",
       "          True, False]),\n",
       "  array([False, False,  True, False, False, False, False, False,  True,\n",
       "         False, False, False, False, False, False, False, False, False,\n",
       "         False,  True]),\n",
       "  array([False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,\n",
       "         False, False]),\n",
       "  array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True])],\n",
       " 'local_rank': [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan]),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])],\n",
       " 'global_rank': [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan]),\n",
       "  array([0.15  , 0.0125, 0.3375, 0.025 , 0.1625, 0.325 , 0.5875, 0.0625,\n",
       "         0.05  , 0.0875, 0.6   , 0.1875, 0.175 , 0.4875, 0.2375, 0.25  ,\n",
       "         0.1   , 0.4375, 0.8125, 0.1375]),\n",
       "  array([0.3   , 0.4   , 0.8625, 0.3875, 0.3625, 0.675 , 0.8   , 0.2125,\n",
       "         0.2   , 0.8875, 0.525 , 0.85  , 0.7375, 0.425 , 0.6875, 0.075 ,\n",
       "         0.9625, 0.9875, 0.    , 0.775 ]),\n",
       "  array([0.975 , 0.6625, 0.2875, 0.875 , 0.475 , 0.825 , 0.625 , 0.7875,\n",
       "         0.225 , 0.45  , 0.35  , 0.7   , 0.6125, 0.925 , 0.9   , 0.5625,\n",
       "         0.5375, 0.55  , 0.3125, 0.0375]),\n",
       "  array([0.275 , 0.9375, 0.9125, 0.8375, 0.4125, 0.375 , 0.7125, 0.1125,\n",
       "         0.95  , 0.75  , 0.2625, 0.5125, 0.725 , 0.7625, 0.6375, 0.5   ,\n",
       "         0.125 , 0.575 , 0.65  , 0.4625]),\n",
       "  array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])],\n",
       " 'margin': [4.440087375713492],\n",
       " 'margin_per_neuron': [0.1480029125237831],\n",
       " 'margin_per_time': [9.922755022660642],\n",
       " 'tightness_hat': [array([0.09424631, 0.11750879, 0.08402265, 0.09928122, 0.11205719,\n",
       "         0.10331195, 0.09183613, 0.10762731, 0.09717738, 0.1020045 ,\n",
       "         0.08931164, 0.08282923, 0.0931716 , 0.07966645, 0.08765065,\n",
       "         0.08595626, 0.09620386, 0.10302472, 0.07514533, 0.09472781]),\n",
       "  array([0.07860437, 0.10154422, 0.10199287, 0.10767147, 0.14738837,\n",
       "         0.09490802, 0.10262546, 0.06659408, 0.07039313, 0.07217707,\n",
       "         0.19926248, 0.0339989 , 0.06870575, 0.19462154, 0.07526756,\n",
       "         0.14088588, 0.02533073, 0.04106367, 0.20245835, 0.03841373]),\n",
       "  array([0.01478669, 0.0937228 , 0.08525035, 0.0648802 , 0.02499973,\n",
       "         0.04047339, 0.02495553, 0.03906354, 0.04929739, 0.09463505,\n",
       "         0.06578877, 0.04922004, 0.06110571, 0.02644852, 0.06043945,\n",
       "         0.04268859, 0.10895564, 0.13635144, 0.04733093, 0.05772513]),\n",
       "  array([0.10332858, 0.06214189, 0.02105265, 0.09022915, 0.13708622,\n",
       "         0.09147884, 0.10596741, 0.11705893, 0.03782266, 0.07729149,\n",
       "         0.14215525, 0.14708977, 0.0823515 , 0.08359697, 0.14057539,\n",
       "         0.1535481 , 0.08936453, 0.14834334, 0.11934375, 0.01127471]),\n",
       "  array([0.1734931 , 0.07504975, 0.14289621, 0.12956035, 0.17432835,\n",
       "         0.16350445, 0.17666961, 0.18194271, 0.20111878, 0.09624411,\n",
       "         0.1227256 , 0.08213027, 0.11843623, 0.18701942, 0.07967911,\n",
       "         0.06280197, 0.15902333, 0.072278  , 0.0908207 , 0.06651466]),\n",
       "  array([-6.39177426, -4.36800585,  0.79552876, -0.01912407, -3.52070023,\n",
       "          1.95168688,  0.61189154,  1.07898203, -2.84009746,  0.21486805])],\n",
       " 'min_tightness_hat': [0.07514533057344241,\n",
       "  0.02533072552861959,\n",
       "  0.014786691971878985,\n",
       "  0.011274705901081594,\n",
       "  0.06280197319984138,\n",
       "  -6.391774260323512],\n",
       " 'max_tightness_hat': [0.1175087878876564,\n",
       "  0.20245835116429145,\n",
       "  0.13635144387556863,\n",
       "  0.15354809576830863,\n",
       "  0.20111877881732254,\n",
       "  1.9516868846100195],\n",
       " 'median_tightness_hat': [0.09448706275679808,\n",
       "  0.08675619669654488,\n",
       "  0.053511259819926105,\n",
       "  0.09740370555533873,\n",
       "  0.12614297250275275,\n",
       "  0.09787199070229526]}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose=True\n",
    "analyse_nn_and_write('mnist_relu_6_20', [2, 10] , 0.001, verbose=verbose, global_rank_threshold=[0.3, 0.0],\n",
    "                    local_rank_threshold=[0.0,0.0], tightness_threshold=[0.3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "imgs = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Number of relu layers: 4\n",
      "Number of hidden layers: 4\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 2.498763\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.55, -0.52,   nan,   nan,  0.  ],\n",
      "       [-0.02,  0.01,   nan,   nan,  0.  ],\n",
      "       [-0.51, -0.48,   nan,   nan,  0.  ],\n",
      "       ...,\n",
      "       [ 1.49,  1.52,   nan,   nan,  0.  ],\n",
      "       [-0.76, -0.73,   nan,   nan,  0.  ],\n",
      "       [ 0.7 ,  0.73,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 364.406077\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.76, -0.29,  0.58,  0.58,  0.  ],\n",
      "       [-0.69, -0.25,  0.82,  0.59,  0.  ],\n",
      "       [-1.77, -1.34,  0.56,  0.82,  0.  ],\n",
      "       ...,\n",
      "       [-0.33,  0.11,  0.75,  0.69,  0.  ],\n",
      "       [ 0.26,  0.29,  0.27,  0.05,  1.  ],\n",
      "       [-0.32,  0.14,  0.81,  0.27,  0.  ]])\n",
      "--------------------------\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 404.542778\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-2.92,  0.98,  0.59,  0.21,  0.  ],\n",
      "       [-2.67,  1.38,  0.5 ,  0.78,  0.  ],\n",
      "       [-2.82,  1.31,  0.44,  0.58,  0.  ],\n",
      "       ...,\n",
      "       [-2.13,  1.73,  0.87,  0.39,  0.  ],\n",
      "       [-1.88,  2.05,  0.39,  0.81,  0.  ],\n",
      "       [-1.1 ,  2.83,  0.48,  0.48,  0.  ]])\n",
      "--------------------------\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 47.409284\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 2.03,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 16.87,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 30.86,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 19.67,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  9.46,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.73,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.67,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 23.52,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 16.63,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 30.73,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "can not be verified\n"
     ]
    }
   ],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=[0.3,0.1], local_rank_threshold=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Number of relu layers: 4\n",
      "Number of hidden layers: 4\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 1.948654\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.55, -0.52,   nan,   nan,  0.  ],\n",
      "       [-0.02,  0.01,   nan,   nan,  0.  ],\n",
      "       [-0.51, -0.48,   nan,   nan,  0.  ],\n",
      "       ...,\n",
      "       [ 1.49,  1.52,   nan,   nan,  0.  ],\n",
      "       [-0.76, -0.73,   nan,   nan,  0.  ],\n",
      "       [ 0.7 ,  0.73,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 337.594208\n",
      "Time per LP neuron 1.096085\n",
      "LP used on 308 neurons.\n",
      "Median global rank: 0.507812\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.76, -0.29,  0.58,  0.58,  0.  ],\n",
      "       [-0.69, -0.25,  0.82,  0.59,  0.  ],\n",
      "       [-1.77, -1.34,  0.56,  0.82,  0.  ],\n",
      "       ...,\n",
      "       [-0.33,  0.11,  0.75,  0.69,  0.  ],\n",
      "       [ 0.26,  0.29,  0.27,  0.05,  1.  ],\n",
      "       [-0.1 , -0.07,  0.81,  0.27,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 389.193127\n",
      "Time per LP neuron 3.778574\n",
      "LP used on 103 neurons.\n",
      "Median global rank: 0.486328\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-2.92,  0.95,  0.59,  0.21,  0.  ],\n",
      "       [-2.71,  1.39,  0.5 ,  0.78,  0.  ],\n",
      "       [-2.84,  1.32,  0.44,  0.58,  0.  ],\n",
      "       ...,\n",
      "       [-2.18,  1.77,  0.87,  0.39,  0.  ],\n",
      "       [-1.9 ,  2.09,  0.39,  0.81,  0.  ],\n",
      "       [-1.11,  2.89,  0.48,  0.48,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 59.117599\n",
      "Time per LP neuron 5.911760\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.98,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 17.77,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 33.03,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 20.15,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 10.39,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.23,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 19.28,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 24.62,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 17.59,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 32.74,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): -32.051506 \n",
      "\n",
      "Margin per LP neuron (more positve is better): -0.076132\n",
      "\n",
      "Margin per second (more positve is better):  -0.040682\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "can not be verified\n"
     ]
    }
   ],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=[0.0,0.0], local_rank_threshold=[0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Number of relu layers: 4\n",
      "Number of hidden layers: 4\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 2.288176\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.55, -0.52,   nan,   nan,  0.  ],\n",
      "       [-0.02,  0.01,   nan,   nan,  0.  ],\n",
      "       [-0.51, -0.48,   nan,   nan,  0.  ],\n",
      "       ...,\n",
      "       [ 1.49,  1.52,   nan,   nan,  0.  ],\n",
      "       [-0.76, -0.73,   nan,   nan,  0.  ],\n",
      "       [ 0.7 ,  0.73,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 132.400223\n",
      "Time per LP neuron 1.285439\n",
      "LP used on 103 neurons.\n",
      "Median global rank: 0.507812\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.76, -0.29,  0.58,  0.58,  0.  ],\n",
      "       [-0.69, -0.25,  0.82,  0.59,  0.  ],\n",
      "       [-1.77, -1.34,  0.56,  0.82,  0.  ],\n",
      "       ...,\n",
      "       [-0.33,  0.11,  0.75,  0.69,  0.  ],\n",
      "       [ 0.26,  0.29,  0.27,  0.05,  1.  ],\n",
      "       [-0.32,  0.14,  0.81,  0.27,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 1239.535645\n",
      "Time per LP neuron 4.024466\n",
      "LP used on 308 neurons.\n",
      "Median global rank: 0.486328\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-1.16, -0.82,  0.59,  0.21,  1.  ],\n",
      "       [-3.17,  1.86,  0.5 ,  0.78,  0.  ],\n",
      "       [-3.28,  1.81,  0.44,  0.58,  0.  ],\n",
      "       ...,\n",
      "       [-2.65,  2.19,  0.87,  0.39,  0.  ],\n",
      "       [-2.34,  2.55,  0.39,  0.81,  0.  ],\n",
      "       [-1.57,  3.34,  0.48,  0.48,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 40.332170\n",
      "Time per LP neuron 4.033217\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 0.5 ,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 19.39,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 34.76,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 20.98,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 12.74,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 17.92,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 20.63,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 25.91,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.55,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 35.81,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): -35.308062 \n",
      "\n",
      "Margin per LP neuron (more positve is better): -0.083867\n",
      "\n",
      "Margin per second (more positve is better):  -0.024961\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "can not be verified\n"
     ]
    }
   ],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=[0.0,0.0], local_rank_threshold=[0.1, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Number of relu layers: 4\n",
      "Number of hidden layers: 4\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 1.722026\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.55, -0.52,   nan,   nan,  0.  ],\n",
      "       [-0.02,  0.01,   nan,   nan,  0.  ],\n",
      "       [-0.51, -0.48,   nan,   nan,  0.  ],\n",
      "       ...,\n",
      "       [ 1.49,  1.52,   nan,   nan,  0.  ],\n",
      "       [-0.76, -0.73,   nan,   nan,  0.  ],\n",
      "       [ 0.7 ,  0.73,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 518.235463\n",
      "Time per LP neuron 1.028245\n",
      "LP used on 504 neurons.\n",
      "Median global rank: 0.507812\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.76, -0.29,  0.58,  0.58,  0.  ],\n",
      "       [-0.69, -0.25,  0.82,  0.59,  0.  ],\n",
      "       [-1.77, -1.34,  0.56,  0.82,  0.  ],\n",
      "       ...,\n",
      "       [-0.33,  0.11,  0.75,  0.69,  0.  ],\n",
      "       [ 0.26,  0.29,  0.27,  0.05,  1.  ],\n",
      "       [-0.32,  0.14,  0.81,  0.27,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 348.459656\n",
      "Time per LP neuron 3.318663\n",
      "LP used on 105 neurons.\n",
      "Median global rank: 0.486328\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-2.44,  0.45,  0.59,  0.21,  0.  ],\n",
      "       [-2.19,  0.89,  0.5 ,  0.78,  0.  ],\n",
      "       [-2.25,  0.71,  0.44,  0.58,  0.  ],\n",
      "       ...,\n",
      "       [-1.64,  1.27,  0.87,  0.39,  0.  ],\n",
      "       [-1.43,  1.6 ,  0.39,  0.81,  0.  ],\n",
      "       [-0.58,  2.31,  0.48,  0.48,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 37.720261\n",
      "Time per LP neuron 3.772026\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[10.81,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  8.36,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 22.87,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 10.8 ,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  1.23,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  9.59,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 11.29,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 15.28,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  8.24,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 21.83,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): -12.058906 \n",
      "\n",
      "Margin per LP neuron (more positve is better): -0.019481\n",
      "\n",
      "Margin per second (more positve is better):  -0.013308\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "can not be verified\n"
     ]
    }
   ],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=[0.5,0.1], local_rank_threshold=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Ranking function axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Number of relu layers: 4\n",
      "Number of hidden layers: 4\n",
      "Size of last hidden layer: 10\n",
      "--------------------------\n",
      "Layerno: 0\n",
      "Time 1.711860\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.55, -0.52,   nan,   nan,  0.  ],\n",
      "       [-0.02,  0.01,   nan,   nan,  0.  ],\n",
      "       [-0.51, -0.48,   nan,   nan,  0.  ],\n",
      "       ...,\n",
      "       [ 1.49,  1.52,   nan,   nan,  0.  ],\n",
      "       [-0.76, -0.73,   nan,   nan,  0.  ],\n",
      "       [ 0.7 ,  0.73,   nan,   nan,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 1\n",
      "Time 313.902566\n",
      "Time per LP neuron 1.019164\n",
      "LP used on 308 neurons.\n",
      "Median global rank: 0.489014\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-0.54, -0.51,  0.29,  0.29,  1.  ],\n",
      "       [-0.69, -0.25,  0.93,  0.83,  0.  ],\n",
      "       [-1.77, -1.34,  0.26,  0.93,  0.  ],\n",
      "       ...,\n",
      "       [-0.33,  0.11,  0.95,  0.52,  0.  ],\n",
      "       [ 0.26,  0.29,  0.96,  0.17,  1.  ],\n",
      "       [-0.1 , -0.07,  0.86,  0.01,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "[Network] Input pixels: 1024\n",
      "[Network] Shape of weights: (1024, 1024)\n",
      "[Network] Shape of biases: (1024,)\n",
      "[Network] Out pixels: 1024\n",
      "--------------------------\n",
      "Layerno: 2\n",
      "Time 336.230859\n",
      "Time per LP neuron 3.264377\n",
      "LP used on 103 neurons.\n",
      "Median global rank: 0.506348\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[-1.11, -0.86,  0.83,  0.1 ,  1.  ],\n",
      "       [-2.73,  1.43,  0.64,  0.44,  0.  ],\n",
      "       [-2.75,  1.31,  0.28,  0.59,  0.  ],\n",
      "       ...,\n",
      "       [-2.17,  1.75,  0.46,  0.96,  0.  ],\n",
      "       [-1.88,  2.07,  0.96,  0.96,  0.  ],\n",
      "       [-1.13,  2.85,  0.01,  0.86,  0.  ]])\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Layerno: 3\n",
      "Time 40.133420\n",
      "Time per LP neuron 4.013342\n",
      "LP used on 10 neurons.\n",
      "Median global rank: nan\n",
      "[LB_hat | UB_hat | global_rank | local_rank | use_LP]\n",
      "array([[ 1.33,  0.  ,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 17.22,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 31.98,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 20.43,   nan,   nan,  1.  ],\n",
      "       [ 0.  ,  9.46,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.64,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 18.81,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 23.73,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 17.72,   nan,   nan,  1.  ],\n",
      "       [ 0.  , 31.27,   nan,   nan,  1.  ]])\n",
      "--------------------------\n",
      "\n",
      "----------SUMMARY-----------\n",
      "\n",
      "Verification Margin (more positive better): -30.649550 \n",
      "\n",
      "Margin per LP neuron (more positve is better): -0.072802\n",
      "\n",
      "Margin per second (more positve is better):  -0.044293\n",
      "\n",
      "----------END SUMMARY--------\n",
      "\n",
      "can not be verified\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'median_tightness_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3a7034b1c43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyse_nn_and_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_relu_4_1024'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_rank_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_rank_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-598ddfdbb6f1>\u001b[0m in \u001b[0;36manalyse_nn_and_write\u001b[0;34m(net_code, img_nrs, epsilon, log_file, global_rank_threshold, local_rank_threshold, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Margin per Neuron (more positive is better): %9f \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'margin_per_neuron'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Margin per second (more positve is better): %10f\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'margin_per_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Median tigthness of hat bounds per layer: %s \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'median_tightness_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Min tigthness of hat bounds per layer: %s \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_tightness_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max tigthness of hat bounds per layer: %s \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_tightness_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'median_tightness_hat'"
     ]
    }
   ],
   "source": [
    "stats = analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=[0.0,0.0], local_rank_threshold=[0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=0.8, local_rank_threshold=0.0, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_nn_and_write('mnist_relu_4_1024', imgs , 0.001, global_rank_threshold=0.9, local_rank_threshold=0.0, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
