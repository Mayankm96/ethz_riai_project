{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../ELINA/python_interface/')\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from elina_box import *\n",
    "from elina_interval import *\n",
    "from elina_abstract0 import *\n",
    "from elina_manager import *\n",
    "from elina_dimension import *\n",
    "from elina_scalar import *\n",
    "from elina_interval import *\n",
    "from elina_linexpr0 import *\n",
    "from elina_lincons0 import *\n",
    "import ctypes\n",
    "from ctypes.util import find_library\n",
    "from gurobipy import *\n",
    "import time\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "libc = CDLL(find_library('c'))\n",
    "cstdout = c_void_p.in_dll(libc, 'stdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for debugging in jupyter notebook\n",
    "from IPython.core.debugger import set_trace #TODO remove at end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layers:\n",
    "    def __init__(self):\n",
    "        self.layertypes = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.numlayer = 0\n",
    "        self.ffn_counter = 0\n",
    "        self.rank = []\n",
    "        self.use_LP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bias(text):\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    v = np.array([*map(lambda x: np.double(x.strip()), text[1:-1].split(','))])\n",
    "    #return v.reshape((v.size,1))\n",
    "    return v\n",
    "\n",
    "def parse_vector(text):\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    v = np.array([*map(lambda x: np.double(x.strip()), text[1:-1].split(','))])\n",
    "    return v.reshape((v.size,1))\n",
    "    #return v\n",
    "    \n",
    "def balanced_split(text):\n",
    "    i = 0\n",
    "    bal = 0\n",
    "    start = 0\n",
    "    result = []\n",
    "    while i < len(text):\n",
    "        if text[i] == '[':\n",
    "            bal += 1\n",
    "        elif text[i] == ']':\n",
    "            bal -= 1\n",
    "        elif text[i] == ',' and bal == 0:\n",
    "            result.append(text[start:i])\n",
    "            start = i+1\n",
    "        i += 1\n",
    "    if start < i:\n",
    "        result.append(text[start:i])\n",
    "    return result\n",
    "\n",
    "def parse_matrix(text):\n",
    "    i = 0\n",
    "    if len(text) < 1 or text[0] != '[':\n",
    "        raise Exception(\"expected '['\")\n",
    "    if text[-1] != ']':\n",
    "        raise Exception(\"expected ']'\")\n",
    "    return np.array([*map(lambda x: parse_vector(x.strip()).flatten(), balanced_split(text[1:-1]))])\n",
    "\n",
    "def parse_net(text):\n",
    "    lines = [*filter(lambda x: len(x) != 0, text.split('\\n'))]\n",
    "    i = 0\n",
    "    res = layers()\n",
    "    while i < len(lines):\n",
    "        if lines[i] in ['ReLU', 'Affine']:\n",
    "            W = parse_matrix(lines[i+1])\n",
    "            b = parse_bias(lines[i+2])\n",
    "            res.layertypes.append(lines[i])\n",
    "            res.weights.append(W)\n",
    "            res.biases.append(b)\n",
    "            res.numlayer+= 1\n",
    "            res.rank.append(np.zeros((W.shape[0],1)))\n",
    "            res.use_LP.append(np.full((W.shape[0],1), False))\n",
    "            i += 3\n",
    "        else:\n",
    "            raise Exception('parse error: '+lines[i])\n",
    "    return res\n",
    "\n",
    "def parse_spec(text):\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    with open('dummy', 'w') as my_file:\n",
    "        my_file.write(text)\n",
    "    data = np.genfromtxt('dummy', delimiter=',',dtype=np.double)\n",
    "    low = copy.deepcopy(data[:,0])\n",
    "    high = copy.deepcopy(data[:,1])\n",
    "    return low,high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perturbed_image(x, epsilon):\n",
    "    image = x[1:len(x)]\n",
    "    num_pixels = len(image)\n",
    "    LB_N0 = image - epsilon\n",
    "    UB_N0 = image + epsilon\n",
    "     \n",
    "    for i in range(num_pixels):\n",
    "        if(LB_N0[i] < 0):\n",
    "            LB_N0[i] = 0\n",
    "        if(UB_N0[i] > 1):\n",
    "            UB_N0[i] = 1\n",
    "    return LB_N0, UB_N0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linexpr0(weights, bias, size):\n",
    "    linexpr0 = elina_linexpr0_alloc(ElinaLinexprDiscr.ELINA_LINEXPR_DENSE, size)\n",
    "    cst = pointer(linexpr0.contents.cst)\n",
    "    elina_scalar_set_double(cst.contents.val.scalar, bias)\n",
    "    for i in range(size):\n",
    "        elina_linexpr0_set_coeff_scalar_double(linexpr0,i,weights[i])\n",
    "    return linexpr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(nn, LB_N0, UB_N0, label):   \n",
    "    num_pixels = len(LB_N0)\n",
    "    nn.ffn_counter = 0\n",
    "    numlayer = nn.numlayer \n",
    "    man = elina_box_manager_alloc()\n",
    "    itv = elina_interval_array_alloc(num_pixels)\n",
    "    for i in range(num_pixels):\n",
    "        elina_interval_set_double(itv[i],LB_N0[i],UB_N0[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_pixels, itv)\n",
    "    elina_interval_array_free(itv,num_pixels)\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "           weights = nn.weights[nn.ffn_counter]\n",
    "           biases = nn.biases[nn.ffn_counter]\n",
    "           dims = elina_abstract0_dimension(man,element)\n",
    "           num_in_pixels = dims.intdim + dims.realdim\n",
    "           num_out_pixels = len(weights)\n",
    "\n",
    "           dimadd = elina_dimchange_alloc(0,num_out_pixels)    \n",
    "           for i in range(num_out_pixels):\n",
    "               dimadd.contents.dim[i] = num_in_pixels\n",
    "           elina_abstract0_add_dimensions(man, True, element, dimadd, False)\n",
    "           elina_dimchange_free(dimadd)\n",
    "           np.ascontiguousarray(weights, dtype=np.double)\n",
    "           np.ascontiguousarray(biases, dtype=np.double)\n",
    "           var = num_in_pixels\n",
    "           # handle affine layer\n",
    "           for i in range(num_out_pixels):\n",
    "               tdim= ElinaDim(var)\n",
    "               linexpr0 = generate_linexpr0(weights[i],biases[i],num_in_pixels)\n",
    "               element = elina_abstract0_assign_linexpr_array(man, True, element, tdim, linexpr0, 1, None)\n",
    "               var+=1\n",
    "           dimrem = elina_dimchange_alloc(0,num_in_pixels)\n",
    "           for i in range(num_in_pixels):\n",
    "               dimrem.contents.dim[i] = i\n",
    "           elina_abstract0_remove_dimensions(man, True, element, dimrem)\n",
    "           elina_dimchange_free(dimrem)\n",
    "           # handle ReLU layer \n",
    "           if(nn.layertypes[layerno]=='ReLU'):\n",
    "              element = relu_box_layerwise(man,True,element,0, num_out_pixels)\n",
    "           nn.ffn_counter+=1 \n",
    "\n",
    "        else:\n",
    "           print(' net type not supported')\n",
    "   \n",
    "    dims = elina_abstract0_dimension(man,element)\n",
    "    output_size = dims.intdim + dims.realdim\n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "\n",
    "           \n",
    "    # if epsilon is zero, try to classify else verify robustness \n",
    "    \n",
    "    verified_flag = True\n",
    "    predicted_label = 0\n",
    "    if(LB_N0[0]==UB_N0[0]):\n",
    "        for i in range(output_size):\n",
    "            inf = bounds[i].contents.inf.contents.val.dbl\n",
    "            flag = True\n",
    "            for j in range(output_size):\n",
    "                if(j!=i):\n",
    "                   sup = bounds[j].contents.sup.contents.val.dbl\n",
    "                   if(inf<=sup):\n",
    "                      flag = False\n",
    "                      break\n",
    "            if(flag):\n",
    "                predicted_label = i\n",
    "                break    \n",
    "    else:\n",
    "        inf = bounds[label].contents.inf.contents.val.dbl\n",
    "        for j in range(output_size):\n",
    "            if(j!=label):\n",
    "                sup = bounds[j].contents.sup.contents.val.dbl\n",
    "                if(inf<=sup):\n",
    "                    predicted_label = label\n",
    "                    verified_flag = False\n",
    "                    break\n",
    "\n",
    "    elina_interval_array_free(bounds,output_size)\n",
    "    elina_abstract0_free(man,element)\n",
    "    elina_manager_free(man)        \n",
    "    return predicted_label, verified_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define operations on abstract domain using linear approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hidden_constraint(model, layerno, z, z_hat, weights, biases):\n",
    "    \"\"\"\n",
    "    This function computes “which side” of the ReLU the pre-ReLU activations lies on.\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - layerno: layer number from which z_hat belong\n",
    "        - z: gurobi variables for hidden layer input\n",
    "        - z_hat: gurobi variables for hidden layer output\n",
    "        - weights: weights for the hidden layer\n",
    "        - bias: bias in the hidden layer\n",
    "    OUTPUT:\n",
    "        - model: gurobi model with new hidden constrains\n",
    "   \"\"\"\n",
    "    # Sanity check!\n",
    "    assert len(z) == weights.shape[1]\n",
    "    assert len(z_hat) == weights.shape[0]\n",
    "    \n",
    "    # add constraint to model\n",
    "    for i_out in range(len(z_hat)):\n",
    "        constr = LinExpr() + np.asscalar(biases[i_out])\n",
    "        for s in range(len(z)):\n",
    "            constr += z[s] * np.asscalar(weights[i_out, s])\n",
    "\n",
    "        model.addConstr(z_hat[i_out] == constr, \\\n",
    "                    name=\"hidden_constr_\" + str(layerno) + \"_\" + str(i_out))\n",
    "    \n",
    "    model.update()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relu_activation_constraint(model, layerno, z_hat, z, LB, UB):\n",
    "    \"\"\"\n",
    "    This function computes “which side” of the ReLU the pre-ReLU activations lies on.\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - layerno: layer number from which z_hat belong\n",
    "        - z_hat: gurobi variables for pre-relu input\n",
    "        - z: gurobi variables for relu output\n",
    "        - LB: lower bound of inputs to a relu layer\n",
    "        - UB: upper bound of inputs to a relu layer\n",
    "    OUTPUT:\n",
    "        - model: gurobi model with new ReLU constrains\n",
    "   \"\"\"\n",
    "    # Sanity check!\n",
    "    assert len(z) == len(UB)\n",
    "    \n",
    "    # iterate over each pre-relu neuron activation\n",
    "    for j in range(len(UB)):\n",
    "        u = np.asscalar(UB[j])\n",
    "        l = np.asscalar(LB[j])\n",
    "\n",
    "        if u <= 0:\n",
    "            model.addConstr(z[j] == 0, \\\n",
    "                        name=\"relu_constr_deac_\" + str(layerno) + \"_\" + str(j))\n",
    "        elif l > 0:\n",
    "            model.addConstr(z[j] == z_hat[j], \\\n",
    "                        name=\"relu_constr_deac_\" + str(layerno) + \"_\" + str(j))\n",
    "        else:\n",
    "            alpha = u/(u - l)\n",
    "            model.addConstr(z[j] >= 0 , \\\n",
    "                         name=\"relu_const_ambi_pos_\" + str(layerno) + \"_\" + str(j))\n",
    "            model.addConstr(z[j] >= z_hat[j], \\\n",
    "                         name=\"relu_const_ambi_hid_\" + str(layerno) + \"_\" + str(j))\n",
    "            model.addConstr(z[j] <= alpha * (z_hat[j] - l), \\\n",
    "                         name=\"relu_const_ambi_lin_\" + str(layerno) + \"_\" + str(j))\n",
    "    model.update()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_linear_solver(model, z_hat):\n",
    "    \"\"\"\n",
    "    This function computes lower and upper bound for given objective function and model\n",
    "    INPUT:\n",
    "        - model: gurobi model\n",
    "        - z_hat: gurobi variable to optimize for\n",
    "    OUTPUT:\n",
    "        - LB: lower bound of variable\n",
    "        - UB: upper bound of variable\n",
    "   \"\"\"\n",
    "    # Find Lower Bound\n",
    "    model.setObjective(z_hat, GRB.MINIMIZE)\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.Status.OPTIMAL:\n",
    "        LB = model.objVal\n",
    "    else:\n",
    "        raise(RuntimeError('[Min] Error. Not Able to retrieve bound. Gurobi Model. Not Optimal.'))\n",
    "    \n",
    "    # reset model \n",
    "    model.reset()\n",
    "\n",
    "    # Find Upper Bound\n",
    "    model.setObjective(z_hat, GRB.MAXIMIZE)\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "\n",
    "    if model.status == GRB.Status.OPTIMAL:\n",
    "        UB = model.objVal\n",
    "    else:\n",
    "        raise(RuntimeError('[Max] Error. Not Able to retrieve bound. Gurobi Model. Not Optimal.'))\n",
    "    \n",
    "    # reset model \n",
    "    model.reset()\n",
    "    \n",
    "    return LB, UB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define operations on abstract domain using Box approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relu_bounds_using_box(man, input_LB, input_UB, num_in_pixels):\n",
    "    '''\n",
    "    This function calculates the bounds of a ReLU operation. \n",
    "    INPUT:\n",
    "        - man: pointer to elina manager\n",
    "        - input_LB: lower bound of the inputs to the ReLU\n",
    "        - input_UB: upper bound of the inputs to the ReLU\n",
    "        - num_in_pixels: number of inputs to ReLU\n",
    "    \n",
    "    OUTPUT:\n",
    "        - output_LB: lower bound of the outputs from ReLU layer\n",
    "        - output_UB: upper bound of the outputs from ReLU layer\n",
    "        - num_out_pixels: number of outputs of ReLI layer\n",
    "    '''\n",
    "    itv = elina_interval_array_alloc(num_in_pixels)\n",
    "\n",
    "    ## Populate the interval\n",
    "    for i in range(num_in_pixels):\n",
    "        elina_interval_set_double(itv[i], input_LB[i], input_UB[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_in_pixels, itv)\n",
    "    elina_interval_array_free(itv, num_in_pixels)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Handle ReLU Layer\n",
    "    # ------------------------------------------------------------------\n",
    "    num_out_pixels = num_in_pixels\n",
    "    \n",
    "    element = relu_box_layerwise(man, True, element,0, num_in_pixels)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    output_LB = np.zeros((num_out_pixels, 1), float)\n",
    "    output_UB = np.zeros((num_out_pixels, 1), float)\n",
    "    for j in range(num_out_pixels):\n",
    "        output_LB[j] = bounds[j].contents.inf.contents.val.dbl\n",
    "        output_UB[j] = bounds[j].contents.sup.contents.val.dbl\n",
    "    \n",
    "    # free out the memory allocations\n",
    "    elina_interval_array_free(bounds, num_out_pixels)\n",
    "    elina_abstract0_free(man, element)\n",
    "    \n",
    "    return output_LB, output_UB, num_out_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_bounds_using_box(man, weights, biases, input_LB, input_UB, num_in_pixels, verbose=False):\n",
    "    '''\n",
    "    This function calculates the bounds of a ReLU operation followed by a hidden layer. \n",
    "    INPUT:\n",
    "        - man: pointer to elina manager\n",
    "        - weights: weights of the hidden layer\n",
    "        - biases: biases of the hidden layer\n",
    "        - input_LB: lower bound of the inputs to the hidden layer\n",
    "        - input_UB: upper bound of the inputs to the hidden layer\n",
    "        - num_in_pixels: number of inputs to the input layer\n",
    "    \n",
    "    OUTPUT:\n",
    "        - output_LB: lower bound of the outputs from hidden layer\n",
    "        - output_UB: upper bound of the outputs from hidden layer\n",
    "        - num_out_pixels: number of outputs of hidden layer\n",
    "    '''\n",
    "    itv = elina_interval_array_alloc(num_in_pixels)\n",
    "\n",
    "    ## Populate the interval\n",
    "    for i in range(num_in_pixels):\n",
    "        elina_interval_set_double(itv[i], input_LB[i], input_UB[i])\n",
    "\n",
    "    ## construct input abstraction\n",
    "    element = elina_abstract0_of_box(man, 0, num_in_pixels, itv)\n",
    "    elina_interval_array_free(itv, num_in_pixels)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Handle Affine Layer\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # calculate number of outputs\n",
    "    num_out_pixels = len(weights)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[Network] Input pixels: \" + str(num_in_pixels))\n",
    "        print(\"[Network] Shape of weights: \" + str(np.shape(weights)))\n",
    "        print(\"[Network] Shape of biases: \" + str(np.shape(biases)))\n",
    "        print(\"[Network] Out pixels: \" + str(num_out_pixels))\n",
    "\n",
    "    # Create number of neurons in the layer and populate it\n",
    "    # with the number of inputs to each neuron in the layer\n",
    "    dimadd = elina_dimchange_alloc(0, num_out_pixels)    \n",
    "    for i in range(num_out_pixels):\n",
    "        dimadd.contents.dim[i] = num_in_pixels\n",
    "\n",
    "    # Add dimensions to an ElinaAbstract0 pointer i.e. element\n",
    "    elina_abstract0_add_dimensions(man, True, element, dimadd, False)\n",
    "    elina_dimchange_free(dimadd)\n",
    "\n",
    "    # Create the linear expression associated each neuron\n",
    "    var = num_in_pixels\n",
    "    for i in range(num_out_pixels):\n",
    "        tdim = ElinaDim(var)\n",
    "        linexpr0 = generate_linexpr0(weights[i], biases[i], num_in_pixels)\n",
    "        # Parallel assignment of several dimensions of an ElinaAbstract0 by using an ElinaLinexpr0Array\n",
    "        element = elina_abstract0_assign_linexpr_array(man, True, element, tdim, linexpr0, 1, None)\n",
    "        var += 1\n",
    "\n",
    "    # Pointer to which semantics we want to follow.\n",
    "    dimrem = elina_dimchange_alloc(0, num_in_pixels)\n",
    "    for i in range(num_in_pixels):\n",
    "        dimrem.contents.dim[i] = i\n",
    "        \n",
    "    # Remove dimensions from an ElinaAbstract0\n",
    "    elina_abstract0_remove_dimensions(man, True, element, dimrem)\n",
    "    elina_dimchange_free(dimrem)\n",
    "    \n",
    "    # get bounds for each output neuron\n",
    "    bounds = elina_abstract0_to_box(man,element)\n",
    "    \n",
    "    output_LB = np.zeros((num_out_pixels, 1), float)\n",
    "    output_UB = np.zeros((num_out_pixels, 1), float)\n",
    "    for j in range(num_out_pixels):\n",
    "        output_LB[j] = bounds[j].contents.inf.contents.val.dbl\n",
    "        output_UB[j] = bounds[j].contents.sup.contents.val.dbl    \n",
    "    \n",
    "    # free out the memory allocations\n",
    "    elina_interval_array_free(bounds, num_out_pixels)\n",
    "    elina_abstract0_free(man, element)\n",
    "    \n",
    "    return output_LB, output_UB, num_out_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to verify the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_network(LB_N0, UB_N0, LB_NN, UB_NN, label, num_input_pixels = 784, num_out_pixels = 10):\n",
    "    '''\n",
    "    This function verifies the network given the bounds of the input layer and the final layer of the network.\n",
    "    INPUT:\n",
    "        - LB_N0: lower bounds of the preturbed input image\n",
    "        - UB_N0: unpper bounds of the preturbed input image\n",
    "        - LB_NN: lower bounds of the final layer of neural network\n",
    "        - UB_NN: upper bounds of the final layer of neural network\n",
    "        - label: true label of the input image\n",
    "        - num_input_pixels: number of pixels in the input image (for MNIST, default: 784)\n",
    "        - num_out_pixels: number of neurons in the last layer of the network  (for MNIST, default: 10)\n",
    "    \n",
    "    OUTPUT:\n",
    "        - predicted_label: label predicted by the neural network\n",
    "        - verified_flag: boolean variable, true if the network is robust to perturbation\n",
    "    '''\n",
    "    \n",
    "    # if epsilon is zero, try to classify else verify robustness \n",
    "    verified_flag = True\n",
    "    predicted_label = 0\n",
    "    if(LB_N0[0]==UB_N0[0]):\n",
    "        for i in range(num_out_pixels):\n",
    "            inf = LB_NN[i]\n",
    "            flag = True\n",
    "            for j in range(num_out_pixels):\n",
    "                if(j!=i):\n",
    "                    sup = UB_NN[j]\n",
    "                    if(inf<=sup):\n",
    "                        flag = False\n",
    "                        break\n",
    "            if(flag):\n",
    "                predicted_label = i\n",
    "                break    \n",
    "    else:\n",
    "        inf = LB_NN[label]\n",
    "        for j in range(num_out_pixels):\n",
    "            if(j!=label):\n",
    "                sup = UB_NN[j]\n",
    "                if(inf<=sup):\n",
    "                    predicted_label = label\n",
    "                    verified_flag = False\n",
    "                    break\n",
    "\n",
    "    if(verified_flag):\n",
    "        print(\"verified\")\n",
    "    else:\n",
    "        print(\"can not be verified\")  \n",
    "        \n",
    "    return predicted_label, verified_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to perform different analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform box analysis for all layers in the neural network nn\n",
    "def perform_box_analysis(nn, LB_N0, UB_N0, verbose = False):\n",
    "    # create a list to store the bounds found through box approximation\n",
    "    LB_hidden_box_list = []\n",
    "    UB_hidden_box_list = []\n",
    "\n",
    "    # create manager for Elina\n",
    "    man = elina_box_manager_alloc()\n",
    "\n",
    "    # initialize variables for the network iteration\n",
    "    numlayer = nn.numlayer \n",
    "    nn.ffn_counter = 0\n",
    "\n",
    "    # for input image\n",
    "    input_LB = LB_N0.copy()\n",
    "    input_UB = UB_N0.copy()\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input Layer, size: \" + str(len(LB_N0)))\n",
    "        print('---------------')\n",
    "\n",
    "    for layerno in range(numlayer):\n",
    "        if verbose:\n",
    "            print(\"Layer Number: \" + str(layerno + 1))\n",
    "\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            if verbose:\n",
    "                print(\"Layer Type: %s\" % nn.layertypes[layerno])\n",
    "\n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.double)\n",
    "            np.ascontiguousarray(biases, dtype=np.double)\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Handle Affine Layer\n",
    "            # ------------------------------------------------------------------\n",
    "            output_LB, output_UB, num_out_pixels = get_hidden_bounds_using_box(man, weights, biases, input_LB, input_UB, num_in_pixels, verbose)\n",
    "\n",
    "            # Add bounds to the list\n",
    "            LB_hidden_box_list.append(output_LB.copy())\n",
    "            UB_hidden_box_list.append(output_UB.copy())\n",
    "            # Prepare variables for next layer\n",
    "            input_LB = output_LB.copy()\n",
    "            input_UB = output_UB.copy()\n",
    "            num_in_pixels = num_out_pixels\n",
    "            nn.ffn_counter += 1 \n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Handle ReLU Layer\n",
    "            # ------------------------------------------------------------------\n",
    "            if(nn.layertypes[layerno] == \"ReLU\"):\n",
    "                output_LB, output_UB, num_out_pixels = get_relu_bounds_using_box(man, input_LB, input_UB, num_in_pixels)\n",
    "\n",
    "            # Prepare variables for next layer\n",
    "            input_LB = output_LB.copy()\n",
    "            input_UB = output_UB.copy()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"[OUTPUT] Bounds: \")\n",
    "                output_LB, output_UB  = output_LB.squeeze(), output_UB.squeeze()\n",
    "                pprint(np.stack((output_LB, output_UB), axis=1))\n",
    "            \n",
    "            if verbose:\n",
    "                print('---------------')\n",
    "\n",
    "        else:\n",
    "            print(' net type not supported')\n",
    "    if verbose:\n",
    "        print(\"Output Layer, size: \" + str(len(output_LB)))\n",
    "\n",
    "    elina_manager_free(man)\n",
    "    \n",
    "    # for last layer of the netowork is ReLU\n",
    "    LB_NN = LB_hidden_box_list[-1].copy()\n",
    "    UB_NN = UB_hidden_box_list[-1].copy()\n",
    "\n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(LB_hidden_box_list[-1])\n",
    "        for i in range(num_out):\n",
    "            if LB_hidden_box_list[-1][i] < 0 :\n",
    "                LB_NN[i] = 0 \n",
    "            if UB_hidden_box_list[-1][i] < 0 :\n",
    "                UB_NN[i] = 0 \n",
    "            \n",
    "    return LB_hidden_box_list, UB_hidden_box_list, LB_NN.squeeze(), UB_NN.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_over_box_approximation(nn, LB_N0, UB_N0, LB_hidden_box_list, UB_hidden_box_list, verbose = False):\n",
    "    # initialize variables for the network iteration\n",
    "    numlayer = nn.numlayer \n",
    "    nn.ffn_counter = 0\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    \n",
    "    m = get_model()\n",
    "    \n",
    "    # We follow the following notations:\n",
    "    # z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "    # z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "    # for input layer:\n",
    "    # Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "    num_in_pixels = len(LB_N0)\n",
    "    img_vars = m.addVars(num_in_pixels, lb=LB_N0, ub=UB_N0, \\\n",
    "                         vtype=GRB.CONTINUOUS, name=\"input_layer\")\n",
    "    \n",
    "    # for output of each ReLU\n",
    "    z = []\n",
    "    z.append(img_vars)\n",
    "    # for output of each hidden layer\n",
    "    z_hat = []\n",
    "\n",
    "    # Create variables for all layers and append to the list \n",
    "    m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "    m.update()\n",
    "    \n",
    "    if verbose: \n",
    "        # Sanity check!\n",
    "        # Size of z should be number of relu activation layers + 1 (for input)\n",
    "        print(\"Number of relu layers: {0}\".format(len(z)))\n",
    "        # Size of z_hat should be number of hidden layers\n",
    "        print(\"Number of hidden layers: {0}\".format(len(z_hat)))\n",
    "        print(\"Size of last hidden layer: {0}\".format(len(z_hat[-1])))\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "    nn.ffn_counter = 0\n",
    "\n",
    "    # Adding weights constraints for k layers\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.float)\n",
    "            np.ascontiguousarray(biases, dtype=np.float)\n",
    "\n",
    "            # add affine constraint\n",
    "            add_hidden_constraint(m, layerno, z[layerno], z_hat[layerno], weights, biases)\n",
    "        \n",
    "            # update counter for next iteration\n",
    "            nn.ffn_counter += 1\n",
    "        else:\n",
    "            raise(\"Not a valid layer!\")\n",
    "        \n",
    "    m.update()\n",
    "\n",
    "    # Adding relu constraints for (k-1) layers. The loop starts from z_2 since z_1 is input\n",
    "    for i in range(1, numlayer):        \n",
    "        # add relu constraint\n",
    "        if (nn.layertypes[layerno] in [\"ReLU\"]):\n",
    "            add_relu_activation_constraint(m, layerno, z_hat[i-1], z[i], LB_hidden_box_list[i-1], UB_hidden_box_list[i-1])\n",
    "\n",
    "    m.update()\n",
    "\n",
    "    # storing upper and lower bounds for last layer\n",
    "    UB = np.zeros_like(nn.biases[-1])\n",
    "    LB = np.zeros_like(nn.biases[-1])\n",
    "    numlayer = nn.numlayer \n",
    "\n",
    "    # Solving for each neuron in the output layer to collect bounds\n",
    "    # i.e. z_hat_{-1} where -1 denotes the last array in list\n",
    "    for i_out in range(len(UB)): \n",
    "        LB[i_out], UB[i_out] = call_linear_solver(m, z_hat[-1][i_out])\n",
    "        \n",
    "    # for last layer of the netowork is ReLU\n",
    "    LB_NN = LB\n",
    "    UB_NN = UB\n",
    "    \n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(UB_NN)\n",
    "\n",
    "        for i in range(num_out):\n",
    "            if LB[i] < 0 :\n",
    "                LB_NN[i] = 0 \n",
    "            if UB[i] < 0 :\n",
    "                UB_NN[i] = 0 \n",
    "                \n",
    "    return LB_NN.squeeze(), UB_NN.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(single_thread=False):\n",
    "    \"\"\"\n",
    "    Get Gurobi model\n",
    "    \"\"\"\n",
    "    m = Model(\"LP\")\n",
    "    m.setParam(\"outputflag\", False)\n",
    "\n",
    "    # disable parallel Gurobi solver\n",
    "    m.setParam(\"Method\", 1)  # dual simplex\n",
    "    if single_thread:\n",
    "        m.setParam(\"Threads\", 1) # only 1 thread\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list, verbose=True):\n",
    "    \"\"\"\n",
    "    Add and create all variables of neural network to gurobi model.\n",
    "    INPUT:\n",
    "        - m: Gurobi model\n",
    "        - numlayer: Number of Layers\n",
    "        - LB_N0: Lower Bound of perturbed image input\n",
    "        - UB_N0: Upper Bound of perturbed image input\n",
    "        - UB_hidden_box_list: List of upper Bounds from box approximation (needed to set upper bound of ReLU outputs)\n",
    "    OUTPUT:\n",
    "        - m: Gurobi model with newly added variables\n",
    "        - z: List of Gurobi variables corresponding to pre-ReLU Layer (hidden)\n",
    "        - z_hat: List of Gurobi variables corresponding to post-ReLU Layer\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # for output of each ReLU\n",
    "    z = []\n",
    "    # for output of each hidden layer\n",
    "    z_hat = []\n",
    "    \n",
    "    # Create variables of input image\n",
    "    num_in_pixels = len(LB_N0)\n",
    "    img_vars = m.addVars(num_in_pixels, lb=LB_N0, ub=UB_N0, \\\n",
    "                 vtype=GRB.CONTINUOUS, name=\"input_layer\")\n",
    "    z.append(img_vars)\n",
    "    \n",
    "    # Create variables for all layers and append to the list \n",
    "    for i in range(numlayer):\n",
    "        # for layers before the final layer, z_hat and z exists\n",
    "        if i < (numlayer - 1):\n",
    "\n",
    "            UB_relu = UB_hidden_box_list[i].squeeze().copy()\n",
    "            for j in range(len(UB_hidden_box_list[i])):\n",
    "                bound = UB_hidden_box_list[i][j]\n",
    "                UB_relu[j] = max(0, bound)\n",
    "            UB_relu.squeeze() \n",
    "\n",
    "            # middle layer, has both z and z hat\n",
    "            z_hat_hidden = m.addVars(len(UB_hidden_box_list[i]), lb=-np.inf, ub=np.inf, \\\n",
    "                                     vtype=GRB.CONTINUOUS, name=\"hidden_layer_\" + str(i))\n",
    "            z_relu = m.addVars(len(UB_hidden_box_list[i]), lb=0.0, ub = UB_relu,\\\n",
    "                               vtype=GRB.CONTINUOUS, name=\"relu_layer_\" + str(i))\n",
    "            # append to the list\n",
    "            z_hat.append(z_hat_hidden)\n",
    "            z.append(z_relu)\n",
    "        # for last layer, only z_hat exists\n",
    "        else: \n",
    "            z_hat_hidden = m.addVars(len(UB_hidden_box_list[i]), lb=-np.inf, ub=np.inf, \\\n",
    "                                     vtype=GRB.CONTINUOUS, name=\"output_layer\") \n",
    "            # append to the list\n",
    "            z_hat.append(z_hat_hidden)\n",
    "\n",
    "    m.update()\n",
    "    \n",
    "    if verbose:\n",
    "        # Sanity check!\n",
    "        # Size of z should be number of relu activation layers + 1 (for input)\n",
    "        print(\"Number of relu layers: {0}\".format(len(z)))\n",
    "        # Size of z_hat should be number of hidden layers\n",
    "        print(\"Number of hidden layers: {0}\".format(len(z_hat)))\n",
    "        print(\"Size of last hidden layer: {0}\".format(len(z_hat[-1])))\n",
    "    \n",
    "    return m, z, z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list, \n",
    "                             LB_hidden_box_list, UB_hidden_box_list, verbose=True):\n",
    "    \"\"\"\n",
    "    Get final bounds using linear programming layerwise. If lp_freq > 1 linear bounds are only calculated every \n",
    "    lp_freq'th layer.\n",
    "    INPUT:\n",
    "        - m: Gurobi model\n",
    "        - z: List of Gurobi variables corresponding to pre-ReLU Layer (hidden)\n",
    "        - z_hat: List of Gurobi variables corresponding to post-ReLU Laye\n",
    "        - nn: Neural Network as defined in initial code (contains layertypes, weights, etc.)\n",
    "        - numlayer: Number of Layers\n",
    "        - LB_N0: Lower Bound of perturbed image input\n",
    "        - UB_N0: Upper Bound of perturbed image input\n",
    "        - lp_list: Layerno to start solvingbounds by LP \n",
    "        - prob: probability to select a neuron\n",
    "        - LB_hidden_box_list: List of upper Bounds from box approximation\n",
    "        - UB_hidden_box_list: List of upper Bounds from box approximation\n",
    "    OUTPUT:\n",
    "        - LB_NN: Lower bounds of neural network output\n",
    "        - UB_NN: Upper bounds of neural network output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity check\n",
    "    assert len(lp_list) != 0\n",
    "    assert LB_hidden_box_list is not None \n",
    "    assert UB_hidden_box_list is not None\n",
    "\n",
    "\n",
    "    # create manager for Elina\n",
    "    man = elina_box_manager_alloc()\n",
    "    \n",
    "    # create gurobi model\n",
    "    m = get_model()\n",
    "    # create all gurobi variables for the network\n",
    "    m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "    \n",
    "    # initialize counter\n",
    "    nn.ffn_counter = 0\n",
    "    \n",
    "    # Adding weights constraints for k layers\n",
    "    for layerno in range(numlayer):\n",
    "        if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "            # read the layer weights and biases\n",
    "            weights = nn.weights[nn.ffn_counter]\n",
    "            biases = nn.biases[nn.ffn_counter]\n",
    "            np.ascontiguousarray(weights, dtype=np.float)\n",
    "            np.ascontiguousarray(biases, dtype=np.float)\n",
    "\n",
    "            # output shape of the layer\n",
    "            n_in = weights.shape[1]\n",
    "            n_out = weights.shape[0]\n",
    "\n",
    "            # create variables to store bounds of hidden layer\n",
    "            LB_hat = np.zeros(n_out, float)\n",
    "            UB_hat = np.zeros(n_out, float)\n",
    "\n",
    "            # add affine constraint\n",
    "            add_hidden_constraint(m, layerno, z[layerno], z_hat[layerno], weights, biases)\n",
    "            \n",
    "            # for initial layers use original box bounds\n",
    "            if layerno < lp_list[0] or layerno == 0:\n",
    "                LB_hat, UB_hat = LB_hidden_box_list[layerno].copy() , UB_hidden_box_list[layerno].copy()\n",
    "            # for the last layer\n",
    "            elif layerno == numlayer - 1:\n",
    "                for i_out in range(n_out):\n",
    "                        LB_hat[i_out], UB_hat[i_out] = call_linear_solver(m, z_hat[layerno][i_out])\n",
    "                break;\n",
    "            else:            \n",
    "                if (layerno in lp_list) or layerno != numlayer-1:\n",
    "                    # find new bounds for the hidden layer using linear solver\n",
    "                    for i_out in range(n_out):\n",
    "                        LB_hat[i_out], UB_hat[i_out] = call_linear_solver(m, z_hat[layerno][i_out])                    \n",
    "                else:\n",
    "                    # find new bounds for the hidden layer using box solver\n",
    "                    LB, UB, n_out = get_relu_bounds_using_box(man, LB_hat_prev, UB_hat_prev, n_in)\n",
    "                    LB_hat, UB_hat, n_out = get_hidden_bounds_using_box(man, weights, biases, \n",
    "                                                                LB, UB, n_out, verbose)\n",
    "            # add relu constraint\n",
    "            if layerno < (numlayer - 1) and nn.layertypes[layerno] in [\"ReLU\"]:\n",
    "                add_relu_activation_constraint(m, layerno, z_hat[layerno], z[layerno + 1], LB_hat, UB_hat)\n",
    "            \n",
    "            # preparation for next iteration    \n",
    "            LB_hat_prev, UB_hat_prev = LB_hat.copy(), UB_hat.copy()\n",
    "                \n",
    "            m.update()\n",
    "\n",
    "            # update counter for next iteration\n",
    "            nn.ffn_counter += 1\n",
    "        else:\n",
    "            raise(\"Not a valid layer!\")\n",
    "    \n",
    "    # Set bounds of last performed layer to output\n",
    "    LB_NN = LB_hat\n",
    "    UB_NN = UB_hat\n",
    "    # If last Layer is RELU change last lower and upper bounds accordingly.\n",
    "    if nn.layertypes[-1] == \"ReLU\" :\n",
    "        num_out = len(UB_hat)\n",
    "\n",
    "    for i in range(num_out):\n",
    "        if LB_hat[i] < 0 :\n",
    "            LB_NN[i] = 0 \n",
    "        if UB_hat[i] < 0 :\n",
    "            UB_NN[i] = 0 \n",
    "           \n",
    "    return LB_NN, UB_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the problem variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_relu_3_10.txt    mnist_relu_6_100.txt  mnist_relu_9_100.txt\r\n",
      "mnist_relu_3_20.txt    mnist_relu_6_200.txt  mnist_relu_9_200.txt\r\n",
      "mnist_relu_3_50.txt    mnist_relu_6_20.txt\r\n",
      "mnist_relu_4_1024.txt  mnist_relu_6_50.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/riai2018/mnist_nets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netname = '/home/riai2018/mnist_nets/mnist_relu_3_10.txt'\n",
    "# specname = '/home/riai2018/mnist_images/img2.txt'\n",
    "# epsilon = 0.01072 NOT verified\n",
    "# epsilon = 0.01071 verified\n",
    "\n",
    "netname = '/home/riai2018/mnist_nets/mnist_relu_6_20.txt'\n",
    "specname = '/home/riai2018/mnist_images/img2.txt'\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nn(netname, specname, epsilon):\n",
    "    flag_wrong_label = False\n",
    "    \n",
    "    with open(netname, 'r') as netfile:\n",
    "        netstring = netfile.read()\n",
    "    with open(specname, 'r') as specfile:\n",
    "        specstring = specfile.read()\n",
    "    nn = parse_net(netstring)\n",
    "    x0_low, x0_high = parse_spec(specstring)\n",
    "    LB_N0, UB_N0 = get_perturbed_image(x0_low,0)\n",
    "    \n",
    "    label, _ = analyze(nn,LB_N0,UB_N0,0) # Get label of unperturbed image, i.e. eps=0\n",
    "    \n",
    "    print(\"True label: \" + str(label))\n",
    "    if(label == int(x0_low[0])):\n",
    "        LB_N0, UB_N0 = get_perturbed_image(x0_low, epsilon)\n",
    "    else:\n",
    "        print(\"image not correctly classified by the network. expected label \",int(x0_low[0]), \" classified label: \", label)\n",
    "        flag_wrong_label =  True\n",
    "        \n",
    "    return LB_N0, UB_N0, nn, label, flag_wrong_label\n",
    "\n",
    "netname = '/home/riai2018/mnist_nets/mnist_relu_9_100.txt'\n",
    "specname = '/home/riai2018/mnist_images/img2.txt'\n",
    "epsilon = 0.01\n",
    "LB_N0, UB_N0, nn, label, flag_wrong_label = load_nn(netname, specname, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n",
      "ReLU\n"
     ]
    }
   ],
   "source": [
    "numlayer = nn.numlayer \n",
    "\n",
    "for layerno in range(numlayer):\n",
    "    if(nn.layertypes[layerno] in ['ReLU', 'Affine']):\n",
    "        print(nn.layertypes[layerno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get perturbed label (provided prediction for unperturbed is true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label: 1\n"
     ]
    }
   ],
   "source": [
    "label, _ = analyze(nn,LB_N0,UB_N0,0) # Get label of unperturbed image, i.e. eps=0\n",
    "print(\"Test label: \" + str(label))\n",
    "\n",
    "if(label == int(x0_low[0])):\n",
    "    LB_N0, UB_N0 = get_perturbed_image(x0_low,epsilon)\n",
    "else:\n",
    "    print(\"image not correctly classified by the network. expected label \",int(x0_low[0]), \" classified label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Find naive/spectral bounds using box approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "LB_hidden_box_list, UB_hidden_box_list, LB_NN, UB_NN = perform_box_analysis(nn, LB_N0, UB_N0, verbose = False)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- For Box ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[ 0.        ,  2.5364939 ],\n",
      "       [ 0.        , 18.05938972],\n",
      "       [ 0.        ,  9.34363425],\n",
      "       [ 0.        ,  6.84798068],\n",
      "       [ 0.        ,  6.22299312],\n",
      "       [ 0.        ,  3.42334266],\n",
      "       [ 0.        ,  0.5614512 ],\n",
      "       [ 0.        , 12.83356247],\n",
      "       [ 0.        ,  9.04202529],\n",
      "       [ 0.        ,  6.93112069]])\n",
      "[VERIFY] Verification status: \n",
      "can not be verified\n",
      "----------------------------\n",
      "Calculation time: 0.001020193099975586 s\n"
     ]
    }
   ],
   "source": [
    "# check verifiability with box!\n",
    "print(\"--------- For Box ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN, UB_NN], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "t = time.time()\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)\n",
    "print(\"----------------------------\")\n",
    "print(\"Calculation time: %s s\"% (t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Find tighter bounds using linear programming over box intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Paper:__ \"Provable defenses against adversarial examples via the convex outer adversarial polytope\"; J. Zico Kolter, Eric Wong [[PDF]](https://machine-learning-and-security.github.io/papers/mlsec17_paper_42.pdf)\n",
    "\n",
    "We follow the following notation: \n",
    "```\n",
    "Consider a k-layer feedforward ReLU-based NN, f_θ : R^|x| → R^|y|. Given equations: \n",
    "        z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}\n",
    "        z_i = max(z_hat_{i-1} , 0),      where i = {2, . . . , k}\n",
    "and, \n",
    "        z_1 = x\n",
    "        f_θ(x) ≡ z_hat_{k}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Calculation time: 0.405013 s.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "LB_NN, UB_NN = perform_linear_over_box_approximation(nn, LB_N0, UB_N0, LB_hidden_box_list, UB_hidden_box_list, verbose = False)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- For Linear (over box) ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[ 0.        ,  0.        ],\n",
      "       [ 4.5207737 , 12.51038959],\n",
      "       [ 1.04261372,  4.46541493],\n",
      "       [ 0.28149916,  3.03626677],\n",
      "       [ 0.        ,  1.32550132],\n",
      "       [ 0.        ,  0.0726665 ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 1.60655949,  7.04623165],\n",
      "       [ 0.        ,  4.62592517],\n",
      "       [ 0.        ,  1.64466924]])\n",
      "[VERIFY] Verification status: \n",
      "can not be verified\n"
     ]
    }
   ],
   "source": [
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear (over box) ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN, UB_NN], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)\n",
    "print(\"----------------------------\")\n",
    "print(\"Calculation time: %4f s.\" % (t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Optimize through linear only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "lp_freq = 1\n",
    "lp_start = 0\n",
    "verbose = False\n",
    "\n",
    "lp_list = [i for i in range(lp_start, numlayer, lp_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relu layers: 9\n",
      "Number of hidden layers: 9\n",
      "Size of last hidden layer: 10\n"
     ]
    }
   ],
   "source": [
    "t_1 =time.time()\n",
    "LB_NN, UB_NN = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list,\n",
    "                             LB_hidden_box_list, UB_hidden_box_list, verbose=verbose)\n",
    "t_2 =time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------TIME------------\n",
      "Total number of layers = 9\n",
      "lp_list = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Time Solving LP: 77.512208 s.\n",
      "----------------------------\n",
      "--------- For Linear ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[0.        , 0.        ],\n",
      "       [4.00119618, 8.26337852],\n",
      "       [0.04321544, 1.35046499],\n",
      "       [0.        , 0.15846734],\n",
      "       [0.        , 0.62339143],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.48459624],\n",
      "       [0.        , 2.40295945],\n",
      "       [1.00752238, 2.57733033],\n",
      "       [0.        , 0.29965822]])\n",
      "[VERIFY] Verification status: \n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "\n",
    "# params\n",
    "lp_freq = 1\n",
    "lp_start = 0\n",
    "\n",
    "m = get_model()\n",
    "t_0 = time.time()\n",
    "m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "t_1 =time.time()\n",
    "LB_NN, UB_NN = perform_linear_layerwise(m, z, z_hat, nn, numlayer, \n",
    "                                        lp_freq=lp_freq, lp_start=lp_start,\n",
    "                                        LB_hidden_box_list=LB_hidden_box_list, \n",
    "                                        UB_hidden_box_list=UB_hidden_box_list)\n",
    "t_2 =time.time()\n",
    "print(\"------------TIME------------\")\n",
    "print(\"Total number of layers = %s\"% numlayer)\n",
    "print(\"lp_list = %s\"% lp_list)\n",
    "print(\"Time Solving LP: %4f s.\" % (t_2-t_1))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN.squeeze(), UB_NN.squeeze()], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Speed Linear Solver up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "lp_freq = 2\n",
    "lp_start = 1\n",
    "verbose = True\n",
    "\n",
    "lp_list = [i for i in range(lp_start, numlayer, lp_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relu layers: 9\n",
      "Number of hidden layers: 9\n",
      "Size of last hidden layer: 10\n"
     ]
    }
   ],
   "source": [
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "\n",
    "# params\n",
    "lp_freq = 1\n",
    "lp_start = 0\n",
    "\n",
    "m = get_model()\n",
    "t_0 = time.time()\n",
    "m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "t_1 =time.time()\n",
    "LB_NN, UB_NN = perform_linear_layerwise(nn, numlayer, LB_N0, UB_N0, lp_list, \n",
    "                             LB_hidden_box_list, UB_hidden_box_list, verbose=verbose)\n",
    "t_2 =time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relu layers: 6\n",
      "Number of hidden layers: 6\n",
      "Size of last hidden layer: 10\n",
      "------------TIME------------\n",
      "Total number of layers = 9\n",
      "lp_list = [1, 3, 5, 7]\n",
      "Time Solving LP: 73.975551 s.\n",
      "----------------------------\n",
      "--------- For Linear ----------\n",
      "[OUTPUT] Bounds: \n",
      "array([[0.        , 0.        ],\n",
      "       [4.00119618, 8.26337852],\n",
      "       [0.04321544, 1.35046499],\n",
      "       [0.        , 0.15846734],\n",
      "       [0.        , 0.62339143],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.48459624],\n",
      "       [0.        , 2.40295945],\n",
      "       [1.00752238, 2.57733033],\n",
      "       [0.        , 0.29965822]])\n",
      "[VERIFY] Verification status: \n",
      "verified\n"
     ]
    }
   ],
   "source": [
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "# We follow the following notations:\n",
    "# z_hat_{i} = W_i * z_i + b_i, where i = {1, . . . , k}, and z_hat_{k} = y (ouput of NN)\n",
    "# z_i = max(z_hat_{i-1} , 0),  where i = {2, . . . , k}, and z_1 = x (input to NN)\n",
    "\n",
    "# for input layer:\n",
    "# Variable lower bound: Note that any value less than -1e20 is treated as negative infinity. \n",
    "\n",
    "# params\n",
    "lp_freq = 3\n",
    "lp_start = 0\n",
    "verbose=False\n",
    "\n",
    "m = get_model()\n",
    "t_0 = time.time()\n",
    "m, z, z_hat = add_all_vars(m, numlayer, LB_N0, UB_N0, UB_hidden_box_list)\n",
    "t_1 =time.time()\n",
    "LB_NN, UB_NN = perform_linear_layerwise(m, z, z_hat, nn, numlayer, \n",
    "                                        lp_freq=lp_freq, lp_start=lp_start,\n",
    "                                        LB_hidden_box_list=LB_hidden_box_list, \n",
    "                                        UB_hidden_box_list=UB_hidden_box_list, verbose=verbose)\n",
    "t_2 =time.time()\n",
    "print(\"------------TIME------------\")\n",
    "print(\"Total number of layers = %s\"% numlayer)\n",
    "print(\"lp_list = %s\"% lp_list)\n",
    "print(\"Time Solving LP: %4f s.\" % (t_2-t_1))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# check verifiability with linear!\n",
    "print(\"--------- For Linear ----------\")\n",
    "print(\"[OUTPUT] Bounds: \")\n",
    "pprint(np.stack([LB_NN.squeeze(), UB_NN.squeeze()], axis = 1))\n",
    "print(\"[VERIFY] Verification status: \")\n",
    "_, flag = verify_network(LB_N0, UB_N0, LB_NN, UB_NN, \\\n",
    "                         label, num_input_pixels = len(LB_N0), num_out_pixels = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn(netname, specname, epsilon)\n",
    "    LB_N0, UB_N0, nn, label, flag_wrong_label = load_nn(netname,\n",
    "                                                specname, epsilon)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
